[Autogenerated] Hello and welcome to plural site. My name is Andrew Malice, and I'll be helping you through the Lenox administration, simplified with said on ork in this first module will be looking at how we can get started with these essential Lennox tools, Francis Bacon once said. That knowledge is power on. What we can look at during the duration of this course is how we can glean this valuable knowledge from our flat files within limits. There's an awful lot of information that is sitting there. We just need to be able to harvest that knowledge. The scenario that we're working with is that Danny has been taken on as a new limits administrator for a D. I y store. Unfortunately, there's been just a little bit too much do it yourself clinics administration. So Danny is going to need to work effectively and quickly to start improving the situation. And then he is going to be implementing grip. So we're gonna be learning how we can use grip that will get us started so we could start to visualize and use regular expressions. These regular expressions are gonna become the main stay off what we use with the main tools said on dork the applications that we start to build, we're better Last log. We're looking at how we can extract XML data and how we can summarize log files. What do you need to bring to this course? Well, not too much. If you need to work similarly to Danny than in the demonstrations will be using up 1 to 14 daughter for the lt s server. But the reality is that any recent Lennox units, all Mac OS X system should suffice. These tools are pretty much standard across the board and have bean for a number of years. This is a stand alone core, so in many respects it doesn't relate back to any other course or scheme. We're looking to develop your skills within the essential tools of said on ork. So as such, you need some basic command line skills. But that's about it. If you're not used to working at the limits command line, then perhaps you can attend the limits command line cli fundamentals course again that could be taken comfortably within your own office or your own home environment, or perhaps even on the commute to work. We'll begin by taking a look at grip and thinking Will walk and Danny teach me about grip. We look at the statement here, declare minus F and then we send the output through to grip. And then we have a little bit of a regular expression going on where we're saying the line must begin with a lower case character or a nun. Underscore. The declare minus F is going to show us all of the functions, but also the function definitions. So we failed to retrieve a single list of function names, even if we use declare minus capital F. That shows me just the function names. But annoyingly, it has declared minus lower case F in front of every function, So that's a little bit annoying. So doing it this way, we beat both the declare minus uppercase and lowercase f by displaying just the function names. Additionally, if you haven't already seen it, we want to check the version of grip that we're using. Then we can use grip space minus minus version. So let's waste no more time and take a quick tour of grip it the command line of Arab one to server
An Introduction to grep

[Autogenerated] you can see now that we're on the command line of our urban to 14.4 server. So this is the long term support version or current long term support version from the A bun. Too stable. We didn't take a look. Now a using great just to make sure that we're up to speed with some of the basics will be using grip later to be able to demonstrate some of the regular expression usage that we're gonna need for our basic tools said and walk. As we run through, we can run command such a cz grip on. We could search they just for user within the TC password file. I'm currently logged in as this'll user. When we look at it, we get a match both on the user user. But we also see system is when I'm running grip here. I'm actually running grip with the option minus minus color auto. So this way we get to see the match very easily. We can add to our string by just putting in front of the U. Let's just put the upper case six on a UK in US keyboard, The carrot symbol this now says that the string more line in our case with grip has to begin with user, We've reduced our search now, so the result only shows the line that begins with the user account on. We miss out on our system user. So already we're starting to pick up on some of the tools that were going to use. When we look at our regular expressions later and how useful and quick this could be implemented, let's just clear out. Move the screen's only use control and l And then if we look at the declare so I can use declare and then minus F to show functions. But you can see the screens rather cluttered. We're seeing both the name and the definition, So there is kind of a solution for that by then looking at declare and minus capital F. But then, annoyingly, it puts the declare miners air for whatever reason, in front of the name of the function. So what we get is is put it back to the declare minus F, and we're gonna pipe the output to our good friend grip on within here. We're just gonna make sure that we don't misinterpret any characters, So it's always a good idea to put your regular expression within their single quotes on. We're going to use thin the carrot symbol to say that the line begins with any lower case characters because all of the functions begin with lower case or an underscore that would be for one of the few, like the system functions clothes off. Then our character code clothes off our quote. And now when we look at it, we get a very easy way of being able to see all of the function names that are defined within my current shell so we can see the absolute difference between the declare minus F on the declare minus capital F and then providing a better declare solution by filtering out all of the definitions off the actual module. And it doesn't really matter if we would do need to look at the details of one of these functions if we use, declare and then minus F, but then put in a function name, so let's go through them, put in, say, quote where they get just the definition for that one function. So I think the solution that we're proposing here will we filter out all of the extraneous information and then drill down to the absolute information that we need is probably the way to go. But it is a great introduction to the power of grip. Unless, of course not forget to check the version of grip that we're using. And, of course, we just use grip minus finest version. And when we look at this, are we using the canoe version of Grip? There are UNIX versions as well. Serve. You are using something. Let's say like Solaris or something like that, then you might not be using the good new version, but we're using the canoe version and version 2.16 Let's jump back now into the slides and take a look at said.
Introducing sed

[Autogenerated] but I'm guessing the name of this course gives it away a little bit. There will be using said and talk a lot in this course, and it also shows the importance of them as tools to the administrator will be looking at here, how we can use, said toe edit configuration files. And they'll be useful for you as well as for Danny. He's noticed that working at D. I. Y R us that there's plenty of automation is going to be needed. There are lots of files that need to be edited across a range of servers, and this needs to be completed accurately, reliably and, of course, on time. Time is always going to be off essence. So looking at this example here that we creating a back up off the NTP dot com and that's done through the minors. I, doc commented. Along with it, we're editing the file and removing the commented lines on the blank lines. So I've actually got two expressions with said here on those expressions are delimited by the semi colon in the middle. Like with grip, we can go and get the version or retrieve the version of said using said, minus minus version. And that's always a good tip. If we're trying to debug issues that we might see perhaps different behavior on different systems. But surely this is enough talking. So let's get in our command line on Look at how we might use set. So we're back on the command line of my 1 to 14.4 machine. I hope that maybe you feel a little bit comfortable now. Working at the command line will first start off by double checking the version of Set that we're using. So as we said the same as with great, we can use minus minus version again, we could see this is a good new version and it's 4.2 dot too. So if I go through and clear out this green, then start looking at some of the things that we can do with said for the moment, we're just gonna have a quick introductory tour. So if I use something like said and then the basics are gonna be the command here, we're gonna use the command p with no other options on against then the argument of the file E TC password. So we could see were really printing all of the users. But if you look closely, each line is printed twice. So with P, we're printing the matching patterns, but as well we get the output from standard out. So we see each line twice if I only want to print not the whole file, just the patterns than we use minus end to suppress standard output. So this time we're actually going to see only those lines that match the pattern. Of course, there is no pattern that we've put in, so it is all of the lines, but it saves us getting the duplicate. So it's important to realize that sometimes that we do need to suppress the standard output. If we want to start restricting what we display than we put ranges in, the range will go inside the single quotes and we can put a range simply like this from lines one through to five. So we're printing if you like the 1st 5 lines, so now will we go through a look at it. We've got those five lines. We could also go through new something like with great where we could go through and say whether line starts with users and playing around with our regular expression. We could see when we're putting in a regular expression that we put the DL emitters for the match with the two forward slashes. And now what we go through and have a look. We're seeing just those lines or the single line. In our case that begins with user. Of course, as we said, the real strength, though, is where we're editing files, but we don't even have to edit it. We can just see the output on the screen and then decide later if you want to use in the minus. I switched to do the in place, edit or weaken writeth e information through to another file. Let's just take a look. Then the example we saw within the slide. So using said, we can go in and open up our quote, quote on, put in our range so we can to first of all, just look for commented lines. So lines that begin with a comment. We can put extra spaces in here if we won't because it's not part of our regular expression. Once we've closed out, then those delimit er's So D is going to be to delete those lines. We want to do it on any TC and NTP dot com. So when we take a look at this, we've removed the commented lines. But as you can see, we've still got those empty lines. So let's clear the screen and our para aqui We can put in an extra expression here, so I'm just gonna put in a semi colon, and then we're gonna put in a new range on we're going to put now our empty lines. So those lines that begin with the end of line marker, so that becomes our range. And then, of course, we want to also delete thes. So now when we go through, we've tidy up that configuration and we've done it quite easily, But we haven't actually changed the file. So if I go through now and take a look at Cat and then e t c slash NTP topped comfort, we could see it as is. If we want to go through and make the edit, then that's where we use then said minus I. I don't put anything after the minus I it won't create a backup, but if I put them So like dot B a k. We're going to create their no backup file before we do the edit. So if I go through now and execute this command, of course I'm gonna need root privileges to be able to do this unless just puts to do in front of this. So we're going to First of all, create the backup on that. Backup's going to be the NTP dot com stop back with then going to go ahead and edit the original file in place. So we put in our password so very, very simply, we almost well, what is actually done If I go back now and take a look at the last cat command we could see Now we've got that tidied up NTP dot com. But let's go back now and have a look that we've also got the backup file. That is theory Jinnah ll file. So this is where the strength of said really starts coming in. We'll leave set for a moment that would look at our introduction to ork
DIY 'r' Us Welcomes awk

[Autogenerated] sharing the limelight with said in this course, is Orc another really powerful tool and anyone to be able to show you how simple it could be to implement some solutions using ORC. When we look at this example, we can see some of the immense power that is available to US tow aid us in gaining extra knowledge here. We're just clearing the network card to be able to retrieve the Mac or hardware address on because he is relatively simple and quick to achieve. As with the other utilities, we might need to query the version of walking. With this. It's done slightly differently. So orc minus Capital W on dhe, then the version sub command having seen it then in the slights, let's move out to the command line of the A boon to machine and start seeing some of the power off walk. So as you can see, we're back on the command line of my open to desktop on this time, we're actually on the test. Stop itself using a graphical terminal. Not that you really will make any difference whether you're connecting virus sshh as we were before, or whether you're working on a graphical or physical console yourself. The first thing that we're gonna look at is retrieving the version off a look that we're using. So if we go through and use book and then minus W and then pull out the version on, we could see that we're actually using Mork, not Orc. Now Mork is just a modified version of orcs. This is quite common to find on any orb unto or debian derivative. The modified just means that includes extra functionality that you wouldn't necessarily find in the standard version. Off walk. Other versions of Linux sometimes will use Gore core the good new walk on again that might have functionality. That's not in the standard UNIX orc command. So hopefully that makes sense. But the bottom line is that we're gonna have extra functionality using more, So we're not losing anything. One. Look at the basic functionality of all. When we can use Orc, open up our single quotes on, then our main body will be in brace brackets so we can choose just to use print and then close the brace bracket and close out the single quotes and then, with no back to the NTP dot com that we looked at previously and just print out the NTP dot com. Now, of course, we're not really going to be using a walk to do simple tasks like that, because ultimately that's what we have the command cat full. But it's just clear the screen to start building this up bit by bit. Remember, we're only introducing you to the orc command here, let me say this is the main block, but we can also have a begin and end block so we could then within our begin block, put header information so I can use them. Begin open up my brace brackets and then we'll just have print and then we'll just have a header that reads NTP dot com. So this is just going to be literally a line that prince, then NTP dot com. We then go into our main block. We print the file as we did before, and we can also additionally include an end block again. The end block is named. Every block of code is within brace brackets. The code that we're going to put in here if we're going to print on, then an internal variable, just the number off the road. When we look at this now, when we hear enter, we could see we got NTP dot com. Far title or header is printed, and then at working as a footer, we have our summary information that says there were 14 lines within that NTP dot com so automatically it's quite easy to start, including extra header and footer information within your output. What we're looking at that in our variable. If we go back and look at our body code again, we're going to have now print and we're going to print the line number. And that will be incremental for each line than culminating in the total of 14 that we print in the end. And we also want to then print. We're gonna use dollars $0 0 represents the whole line. When we look at this now, we could see that we've included line numbering ZZZZ we go through. So we've got one through to 14 and then the summary information showing the total number of lines that have bean processed. That gives us a little bit of some ideas of what can be done. The last thing that we have to look at is just the i f conflict commanders we saw within the demonstration that we could use. I have confident we can print out information relating to different network carts, but included in this we have a line than that includes tthe e hardware, the dress. So if we look at filtering just the Mac address, we can have our F conflict and we'll pull it out. Let's say just for one of my car so e th zero was Send the information through to walk. We've got to set their field separator, and I'm going to use a colon here. The colon then appears between each element off the Mac address. I'm just putting this in double quotes, so we're saying that we're using the colon. Is the field the limiter? And that's a minus capital F. We open up. Then the single quotes and in front of the brace brackets will say, Look, I only want to work on lines that contain H d. D. So that's the string that we're looking for. We're going to our main block of code, and then we need to print certain field. So field three field for Field five, Field six and then field seven, close off the bracket. Close off the quote. Of course, we're not referencing a file. Now. The input is coming from standard in from the output off f come thick. So you see there that we've got the Mac address. And of course, if we go up and change us, then to e. T h one, we see the Mac address for E T. H one. We can go further with this When we're looking at printing, we can include an internal function so I can use to upper and then put brackets around all of the elements. So my ordinary round parentheses. Now, when we look at it, we've got an uppercase version off the Mac address. So there's lots of things that we can really start benefiting from by using ork. Ork is its own programming language itself and contains a lot of really good tools that we're going to see is we start going through these modules. So let's drop back to our slides now and summarize what we've looked at in this module
Summary

[Autogenerated] Now, of course, we should be respectful to Danny's time, and he is a busy man. Is, you could imagine. Haven't taken on this new role of D I. Y r us. Let's quickly get this more Jule wrapped up, and then we'll be able to move on to the next task in this section. We've introduced Danny and the course to you. We've seen how we can use Grab a quick introduction to grip and the same with Said So introducing these commands along with the powerhouse of Tha Mall Walk next up. We're going to take a look at how we can use grapes and looking at grab him or detail on. We're going to apply it to an application where we're going to pass information from C S V files. So I look forward seeing you in the next module
Using grep
Introduction to the grep Command

[Autogenerated] welcome to this plural site presentation. My name is Andrew Mallet, and in this module will be looking at how we can use grip within our fictitious organization that we're working with in this course scenario. D I Y r us Grip in itself stands for the global regular expression on print were generally going to call it just grip. But it allows us then to search text that we might pass in through standard input or through a named file pass a regular expression which might just be text. Or it might be something simple, like saying the beginning of our line or an end of the line or something a little bit more complex, and then print that, and that's going to be printed out to our screen or standard output. The requirements for grip within the organization has been identified by Danny, so we need to search configuration files for settings so he can easily identify what is set on what currently isn't set. Hardware is mainly undocumented within the organization, and Danny wants to be able to document at least the number of CPU cause within each server to get an understanding of how powerful the servers are and how they're going to cope with the workload. The big application that we're working with to be able to process catalog data is stored within C S V files. We'll see how we could use a bash script along with grip toe. Help us with that. We're looking at the output here. There are a few lines that give us some ideas of how we might use it. We could use I f come Fick e t 80 And then we congrats for Rx so we can see the traffic that's being sent from the machine and received if we want to search a multiple files so you might want to look at the Pam configurations, the plug Kable authentication modules. They include Thea Passion Pam underscore. No log in, but we won't know which file to search so we can search multiple files. And if we simply want to count the number of cause within the CPU, then we can use grip minus c, look for the name and then count. How many occurrences of that are in the file to fundamentally grip is going to allow us to search our files or output from another command. And as we see that as we go through, the first command is showing us our transmission data. The second command then is going through showing us our Pam configurations. And then finally, as we saw before, we're counting the number of CPU cause within our host. So that's enough of seeing slides. Let's see this in action on are a boon to 14.4 desktop.
Understanding the Basics of grep

[Autogenerated] So even though Grip doesn't make up part of the course title, it's gonna help us understand the regular expressions in the next module. And in itself. Grip is a really useful tool, so we'll be taking a look at some of the functionality of Greg. As always, we can go into the man page or just the help options to see everything that's available. So I run grip and minus minus help. We get some summary information of spot of the commands that are available. We could also go into the man page, of course, so we can go man grip to get Maur detailed information. But I'm getting you're watching the video because you want to be shown rather having to read through these pages of information. Let's get straight into looking at some of these options, so get going back to the I F. Conficker command we can use. I have confidence. Let's use it gives e th zero, and if we go through and then just search for our X so we could start seeing some of the transmission information. So the number of packets that we've received But we also could see in the second line the number of bite we've transmitted. So this is a great indication whether or not we've got a healthy network card by the number of packets that perhaps are being dropped. If we go through, have run this again. And perhaps we just put a lower case R x, we don't get any output, but we can use grip minors I for in case sensitive search. So we're uncertain of the case of adoption that we're searching for or the text. Then we can use grip minus I. We could also use grip minus V, and then we're reversing the search. So we're not looking for the lower case R. X, so anything that contains a lower case r X, we're going to exclude. However, these all of course don't ex include the lower case R X. So pretty much we're seeing the full output of I f come thick here. So they're just looking at some of the basic options. Sometimes, though, when we're looking for configurations, we're not certain which configuration file it's going to be in. So we could then use grip to search for maybe the Pam underscore. Log in, module on. We can take a look, then at the configuration files that it could be stored in. So it's going to be within the pam dot de directory on. We could just use Asterix here to say, Look in a LL files on now we're busy work, eh? It's stored in the pam dot d s s h d file on. Then we print the line. So this is a great way to be able to locate a set setting when we're unsure which file it might be included in if we go through and look at grip a minus C, The lower case C is used to count the currencies rather than display them. And we look at name and look at that from Prock and CPU info. We can see on this machine that we've got two processes because there and take a look at that file. So if I go back and look at the last argument, we can then go through and see that for reach process er that we have. We have a model name field, and that's really what it's looking at here Got process of one. If we scroll back up and we'll have process er zero So again, we're very easy being able to retrieve the number of course or processes within this host. Let's jump back now into the slides and take a look at it now the application that we want to be working with our past C S V. So we could look at how we can use grip to process information from comma separated files.
DIY 'r' Us Application: Parsing CSV Files

[Autogenerated] looking at the script that's being developed by Danny for D I Y r us. We could see that it is a basic shell script. The first line is just say we're going to run with the interpreter forward slash been four slash bash. We then store the current field separator If there is one in the shell into a variable called old I f. S on. We set the new field separator to be a comma. Remembering that we're using a comma separated file here on we need to set the correct delimit er within our wild loop were looping around wells were able to read in for each line the three field So product price on quantity, This makes up if you like the schemer of R. C s fi file within the dew and done block the echo statement, we can see the 1st 1 is just using some color coding. So we're setting, if you like the text to yellow So that's easily seen and were displaying the product name at the end of this string. We're just setting the text back then to the normal black. We then print out the price and the quantity the slash e in the echo statement is just in an escape. Forward slash t is a tabard forward slash end. You probably know as a new line. We read into that the first parameter that we passed through to the script on we reset the current field separator back to the old field separator, of course, grips not used within this. But if we just run, this script is gonna print out the complete catalogue so we can implement Grete by using grip minus capital a two on, then search for what? Everyone here. We're searching for hammer so we can use grip to display individual catalog entries. We need three lines for each entry. That's going to be the name, the price in the quantity. So we're searching for the name and we want to show two lines after the line that we search for. So you get three lines in our Alpert, the line that we search for and two lines afterwards. Hopefully, that's clear, but we can make it a little bit more clear to you by looking at the catalog application itself from the command line. So we're back of a command line there knows the board into desktop on 1st 4 Let's just check the script. So here we've got our script past C s v dot Shh. On we could see that is pretty much the same as we had within the slight. Once you're happy with this, let's go through Look at one of our catalogs. So we cat out the tools file so this thing can act as our input. And if we clear the screen and then see how we could use this So running from the current director, we can go past CSP veggies here, enter on that. You can see at the moment that we haven't read in our input file, of course, it's requiring the input file. So if we put in one of our catalogs here, we're putting in, then tools we could see then that we get their catalog printed out through this is just a demonstration catalogs. We don't have all of the information, and we're gonna migrate across to our production systems once it's bean fully tested. Because see, then that we get the output as such. The product line itself the product name. So the drill hammer brush, we could see we've highlighted, then in yellow. And then we're back to our normal text when we print out the price and the quantity. Because if we had many entries in here, hundreds or thousands, it's not going to be particularly useful by itself. So if we go through and run that and then using an unnamed pipe, we can send the output through to grip and then minus capital A too. So you want to lines after the line that we search for. We're always going to get the line that we search for as well and then put in hammer. We then get just the output for hammer. Don't worry about the output now appearing as red. We're no longer really using the script to output. We're using grip toe output. If I go through and take a look at my aliases, we can see that I've got an alias in place for grip on. This is common on a few systems that uses grip minus minus. Color equals auto. So the search string is highlighted in red, but again clearing the screen, going back, running the search. We could see now that we're able to isolate each entry and be able to effectively print the result that we want now. Of course, the minus capital a option is just one off the context options. If we chose B for before that, we see the current line and two lines before, if we chose Capital C, remember Lower Casey was count Upper Case C is context and is going to show two lines before and two lines after all. Really, this isn't what we require for this application, but you can see how you can adjust your own grip sequences to work with the data that you are currently working with, says a very flexible command. And don't forget as well. We can combine many different options together so we could start adding, in case insensitive. And then if we put in actual an uppercase hammer, we still get the match. Whereas we tried it before without the miners, I Then, of course, we don't match now because there's no string that spelt hammer with a Capital H but returning to our application. This is the result that we need, and Denny is quite happy with this. So let's jump back now into the slide on wrap up this section
Summary

[Autogenerated] So we've seen how well Danny's been doing at D I. Y r us, and he's progressed to the next level. We need to review how he managed to get there during this module. We've taken a look at how we can use grip to list our hardware detail. Remember, we did that by looking at the I. F Conflict Command on. We were searching for the Mac address and the receive and transmit detail. We were searching through Pam Configurations for specific settings. So that's where we then use the Globe ing character star so we could search all files within the e. T. C pam dot d on we implemented the catalog application, so we used a bash shell script for that. But within the application, we use grab minus capital A to display lines after the found result, as well as the result that we were looking for in that way, we could. Then display is we saw hammer with the price and quantity that we had in stock. So be prepared now to fasten your seatbelts, we're gonna have a jolting right straight into regular expressions
Regular Expressions
Mastering Regular Expressions

[Autogenerated] welcome to this floor site presentation. My name is Andrew Mallet, and we're gonna take a look now through regular expressions. So when you're quite ready, will take a look at this. And, yes, I can understand how you're feeling, but each one of these characters has a special meaning to regular expressions on. We're going to be looking at decoding or, if you like, trying to make sense off all off this spot. So Danny needs to understand an implement, regular expressions, and in doing so, he will understand. Anchors, ranges, boundaries quantifies, and we'll end up by validating data. We're gonna have an employee file that we have to work with with D I Y r us on. We want thio identify those employees who don't have a valid Social Security number, so regular expression is gonna fit nicely into the equation. Here. A regular expression itself is just a sequence of symbols and characters expressing a string or pattern to be matched within a longer text. Let's see what we mean by this. So this is a regular expression. The options that we using slash b at the beginning and the end is just saying there's a word boundary, so we're looking for the word color, but we don't mind whether the sea is in uppercase or lower case, so that's a range specified by our square brackets. Of course, it might be spelled in the American format or the English format, so we can make a character optional by including the question mark after it. So you question Mark means that we can have zero or one you following the O and before the R, so we're matching on color, color, color, color. When we talk about anchors, we can see the use off the carrot, meaning the beginning or start of a string so we could search for route being the beginning off the line or string. The dollar symbol, then, is correspondingly at the end off the string. So in this case, we're looking for the character for being at the end off the string. Let's jump into the command line now of are a boon to desktop and see how we can use thes anchors on again whilst with demonstrating regular expressions were going to be using grip, although future demonstrations will be able to see those same regular expressions with orc on Dhe said
Using Anchors

[Autogenerated] so we can see working at our command line of the A bunting machine. How we could start using our regular expressions on this would be the same pretty much on any version of the UNIX OSX or as I'm using here, open to. But this really shouldn't matter the distribution that we're using if we go through and look for something like grip, let's go through and look for server. We're gonna look for this in the e. D. C and n tp dot com for a link. I throw a look in the original one that we backed up there. We see when we go through and Macha we're matching on server, not just the beginning of the line, but we're seeing matches further in the line as well. So we want to restrict our search just to the occurrence of server, the beginning of the line. That's where we can start to implement anchors. So if I go through now and just put the anchor in front because we're starting to use some special characters, generally a good idea to ensure that the string that includes the regular expression is then quoted in single quotes. So now We're looking for server at the beginning of the line and you can see then that's given me exactly what I required in the same way we have the anchor for the end of the line, and that's the dollar symbol. Very often, this is where we want to see the value of a setting. We perhaps no, the configuration name. But we want to see the options or values that actually set. So if I go through and we're going to grip and we can go through and gripped, let's say for the number four and being at the end off our line. So we're gonna use the dollar and of course, we'll just put their single quotes back around this on. We can go through and search from E T. C. And then we'll have the log rotate dot de directory, and we look, it'll files within there so we can see here. We're rotating certain files, and we've got rotate for written a few times here or four times. Nothing to do with the fact that we search for, for it just happens to be in these files four times. So once occasion in each of these separate four files. This is just specifying if you like, how many backup copies we keep when we rotate log files. So this way we'd be able to implement a search off values by looking for four being the end. If we want to go through and reverse our search to see where we don't have four at the end, then we can go back and use grip and minus V. So this any show where we don't have four at the end. But of course, and this is showing me mainly settings, not just, of course, the rotate settings so we can clear out. We will see with ranges how we can implement a range of numbers that we want to search for, perhaps at the end of the string, going back to the and to ___ back up that we were looking at before we can search for empty lines by implementing than our regular expressions where the line starts with. So I'm going to use then the carrot symbol and then dollar, where the line starts with the end of line marker. So this is our blank lines we can go through and then put e TC NTP dot com dot back So this is an only showing the empty lines. But of course I want to see everything but the empty line so I can use grip minus V from of these reversing our search that shows me everything but empty lines. We can really see what's going on here when I go through and use cat mine. If e e t. And then look at that last argument on, we're going to see now that lines that are blank do, in fact start with a dollar. When we look at the end of the lines, such as the broadcast line, we could see their end to $55. So it becomes very easy to be able to extract this information that exists at the end of the line. Let's return now to the slides and want to begin an understanding of how we can work with ranges within regular expressions.
Working with Ranges

[Autogenerated] within our regular expressions, we can implement ranges or character classes using the square brackets. So if we look at something like this, the square brackets denote the range. The hyphen included is then just saying through to sew it okay at Capital A through to capital said We're also including lower case A through to lower case said So this is in fact, looking for any letter character on. We might decide that we need is the start of the role perhaps two digits in or something like this here. We're looking at the same type of thing, but we're looking for a new miracle data. So we're looking for any digit we could hear like this. But sometimes in a regular expressions we can use forward slash d to represent a digit character. Well, we're looking at our declare minus surfer, looking at ways that we could extract just the function names we use. Then a character classle range similar to this Well, we knew the function was going to beginning with a lower case character, so a through to said in lower case. But also it could begin with an underscore. So here, if you like, we've got the 26 characters of the lower case L for Bette. And in addition, we've got an underscore character. So this includes any lower case letter or underscore here. When we look at it, we're looking for 34 or nine. So not so much. 349 But looking for 34 or nine course, it would also match 349 because it includes 34 and nine. We have to be careful the rest of the expression in case we needed something directly after that. So just be careful when we want to search for multiple numbers. We have to be very careful the way they're expressed. Let's take a look at this now on the command line of the upon to machine to see how this is all going to fit together. So we're working back at the command line. Now we're going to run through a look at some of these ranges. So where we were looking at our declare function before. So if I use and declare and then minus f, we get to see if you like everything about the functions, including the names, as was the definition But all the function names are in lower case or start with underscore, so we can then send the output of this through to grab on. We can search with grip both the file and our standard input. And then we could say Well, the beginning off the line. It must be then within this character classle range whatever you like to call it lower case character or on underscore, close out our quotes. And now when we look at it, sure enough, we're only showing the information where the line begins with the underscore or a lower case character. So we could see very easily how those ranges can help with us. We can also go through and start looking at things where we were looking at before. So I use grip. Now we're going to look for rotate and then a space, and then within this, we can have ah, four or six. We're not looking for 46. We're looking at four or six being then at the end of the line. And then we can close this and go through. Take a look at e t c slash log, rotate doctor and all files within there so we can see now that we've got rotate for rotate six. If we go back up, we could say we want rotate, but we don't want it ending in four. So using the upper case six or carrot symbol with inside the range negates that character. So now when we look at it, we get rotate seven rotate to rotate six, rotate seven But we're not getting the rotate for so again. We could start implementing our grip searches to really then hone down. So we were saying that rotate for means that we're keeping a month's worth of data. So show me all configurations that are not keeping just that months worth of data. And of course, we can use these ranges so we don't have to implement a completely case insensitive search. But there might be just one word that we don't mind the case off a one character so we can use, Then grip and we can go through and search for capital s or lower cases and then server. And we can implement this search through two. Then r e t c slash in t p dot com Stop back. So in this case, that we will be looking for the word server or the word server with the uppercase s in this case is still just returning. Server spelled in lower case. There's another occurrence of server in upper case. When we get the idea of how we could implement this, let's jump back into the slides and look at how we can implement some special characters so we can start looking at word boundaries, spaces, those sorts of things. This way we can isolate individual would rather than seeing as we're currently seeing on our match for the word server, we're getting the S for servers as well.
Identify Boundaries with Strings

[Autogenerated] when we're searching for words. If we want a complete word, we could start looking for boundaries that surround the string so we can use something like slash s that represent white space. So rather than just say, we require a space where that space might be a tap or it might be a line return, so it's much easier to be able to use something like slash s that represent any white space Character slash b represents a word boundary. Which game might be a space but also could include things like a hyphen as a word separator. So word boundary congee shown by slash Be incidentally, if we upper case the B or the S, then we're not looking for a word boundary or we're not looking for a white space, so we reverse the meaning off that boundary. So just using a capital letter instead of the lower case reverses the meaning. So if we have something like Backslash s system than that matches file space system also we had filed tab system it would match. That was we used the word boundary. It would match file space system or file hyphen system again is choosing the correct boundary separator for our regular expressions. We don't jump onto the command line now so we could start seeing some of this in action. So let's begin by returning to the grip search where we're going to look for the string server within our NTP dot com says We go through and used the old backup file. We could see that we're matching the plural of server A CZ well as server. So if we change the search just a little bit, so if I try and put, say a word boundary here, of course, this is where we've definitely got to go in and start using our quoting mechanism. But if we give this a world we can see now that we've been able to exclude the match is on servers. But of course we've still got server. That's not on the beginning off the line, so we could in fact clued a word boundary here. But because all of the words start with server, that's not going to eliminate the problem. So again we can go back Thio putting in then the start of the line, to be honest in this case, because we then don't have servers starting our line. It's a ll that we really need to reduce our search. But we could start looking at how that can be used as well as using the word boundary here. We could also go through and say, Well, server and space And of course, that's going to reduce it for a say anyway, because we were looking at the start of lime of we take that out, Then we could see that were excluded the first line that we had, where we had servers showing. So we've only got occurrences of the word server. If we change this slightly, saying that we don't want a space at the end to using the upper case s now we're only seeing the plural of servers again is just show you how flexible the regular expression can be. But how on the ball you have to be to realize all of the combinations we go through starting to look for something else where we go through, let's say and grip for the string pop. In the DC Service's file, we could see when we gain the result. Now that we're including things like K pop and pop passed on DPA past D. So we've got those twice for TCP and UDP a CZ well as the K pop. If we only want let's say Pop too and pop three and maybe no even pop three s showing that what we can do is clear out the information little bit. Move back up and then we say that we want then the word pop. So we could say that we want a word boundary before this and we could say Then, after this, we need a zero through to nine and then another word boundary. Remember when we put the ranges in for the character class were only including a single character there. So even though we're looking for zero through to nine, then we can have one. A current of a character but one only who wanted to numbers would have to use a quantify or put the brackets in twice. Now, when we go through and search, it's making sure that we close off the quotes. We could see then that we've got popped to pop three showing it showing twice because we got TCP and UDP. So we've adequately shown our word boundary. But again, if we go back up and then say Put a capital B in front here and then go through. Look, we don't have anything that doesn't have a word boundary before the pops There. We can repeat this by trying the capital B at the end. And there we can see. Now we've got popped three s. We don't see the pop to hear. We don't see the ordinary pop three without the character following. So again, we've been able to reverse our search, so using our boundaries can be quite useful. But again, it's just getting used to the complexities of the language on practicing. It's like anything practice does make perfect. Let's jump back now to their slice and take a look at quantifies. We told that quantifies could be useful here. Say, if we wanted multiple numbers or whatever we require, we could specify how many of those that we want. We also saw it with the use of the word color before using the question mark to make the U optional. So we could have the U. S. And the UK spelling of those words were now going to see those quantifies in action. First we'll start with the slides and then watch about to the command line again.
Working with Quantifiers

[Autogenerated] within a regular expression. A quantify specifies how many of the previous characters are allowed, all required. So if we use something like you followed by the Asterix or star character, we're matching you zero or more times so we can have a you know, use or any number of use where we use you and question Mark. It's the same type of thing, but we're saying that we can have zero occurrences off the you or one, but we can't have any more words with the star were saying zero or as many times as we like with the question mark were saying zero or one. That's where we really making the U Optional where we use the plus quantify rhe were saying where we want one or more currencies and where we do something like you and then the brace brackets on a number inside. We're saying we're matching exactly three occurrences off in this case you so we can use this than to specify maybe with a space where we use slash s and then we specify wound might be one or more spaces or we're looking at trying to locate commented lines within a file. Sometimes those comments are spaced in, and sometimes those spaces could be any amount of spaces or tabs. So it's quite useful from that instance, being able to specify the number of objects that we can match. Let's go through now. Work on the command line, actually put some practical basis behind these qualifies. Working in the command line, we can go through work out how we can use some of these quantifies. I just look at my test file here, so if I just cut out the test for you could see that I've got three, commented lines a commented line. Just start with the hash symbol, but we could see the starting line has got a couple of tabs. Then when we moved down towards the middle of the file, we have one space and then the comment on the double B, and then we have the end marker at the end of the file, so it could be awkward trying to work out these comments if we needed to remove all off the commented lined. But if we use our grip minus V because because we don't want to display these commented lines open up, then are quotes to start our search or our regular expression on, we can say that we still want it to start on. We could say that we want to start it with a white space character. So spaces or tabs? But we could say, Well, look, I don't care. We could have zero spaces at the beginning, or we could have 1200 spaces at the beginning. But it has to be followed by the hash symbol, so we can then close off the quote, look through the file and now will we look at it. We're not seeing any commented lines, so this becomes a really reliable way, irrespective of how the comments have been embedded into the file of removing those comments. What? We're looking at the text here, a cz Well, we can see that we have some of the information that is start space, end, start space, space end and start and end. So we want to try and work out how we condemn display those. Then, of course, we can go through and do something like grip and just search for Start on. We're not gonna have any problems. Then be able to show those lines. We can then go through and say, Well, okay, we want tohave start space end. And of course, we didn't just get the one line coming through, but we want Then start an end, Whether there's a space there or not, we can then try and put the slash s in here for our white space and then specify the quantified to say, Well, I don't care how many. So now we're going to see start end as one word on, we're going to see all of the start and ends with spaces in between. So we know what we say here that we're making the space optional, but there is a space. We're saying it could be any amount of spaces if we move up and changed this and put the question mark in with still making your optional. But we're only allowing there to be a maximum of one space. We look at the result there. We're not getting anything coming back, so we change this to grip minus Capital E. We're now looking at an enhanced regular expression. Sometimes this is going to be needed Occasionally people think that we would run IEG grip to do this, but in Linux egress has Bean depreciated. It's still there, but when we start looking at the man, pages say it's being depreciated in favor, off grip minus capital E for the enhanced regular expression. But either way, when we look at this we're seeing now that we're just getting the two results, start space end and start on, then end. And we could go through and Maura less do what we like in here so that I could go through. Let's say we want to see exactly the two spaces. Then we go through and put the quantify to in our brace brackets and, of course, have our slash s before again. This requires an enhanced regular expression. So now we've got start space space end. But this wouldn't work if we were trying to look at three Spaces, will only see the too spaced out start and end, so our quantifies can really begin adding a lot of information to us, especially where we start to want to look at information in a particular way. So we might want to be looking. Let's add a postcode and saying that we need a least two characters. So for that we could go through. Let's just go through and add in our post close. If I just go and look at my test here and we go to the end off the files of Capital G and then just insert a new line after this, if we then have a B 12 7 a f and then we could have a and then we put three numbers and then seven he f something like that with the A B should match a postcode, but the A 234 shouldn't. Now when we go through and go grip, we're going to say for our regular expression that we want then, eh? Through too said. And we'll say that we want at least two off these, so we require, too. And then we could say for load by zero through to nine on then our postcode that they could have one or two off these. This needs to be grip minus capital E. Of course, we need to close out our quote and then go through look at our test file and then we see we pull out with one return, but we don't get the others. The nice thing about regular expressions that you might think there is a lot to learn, but once it has been learned than the regular expression is going to be compliant across any system that can use it. So whether you're using PERL or Python or even Power shell in Windows, the regular expressions that we write are going to be the same. Just jump back now to the slides and take a look at the application that will be working with for D I Y r us on what Danny has to perfect.
Building an Application Process to Validate Employee Records

[Autogenerated] we're now going to prepare ourselves for the application that we're going to use for this part off the course and within this module. HR have asked Danny to identify invalid employee records. It would appear that some employees do not have a valid Social Security, not number included within their personnel records. So Danny needs to be able to list those records that don't contain a valid Social Security number. When we look at grab here again, we're using the minus V to reverse our search on the mind ist capitally. Remember the minus capital E. He said that we using Indian hands regular expression. We're gonna need that for the quantifies that we're using. Now, of course, we could go through and take a look at these by going through the slide, but it's gonna be a lot more practical, and I think you could get a lot more out of it by jumping straight out To are a boon to desktop on having a look at how we can validate these SS ends. So welcome back to the command line of our abode into desktop, and we're gonna run through and take a look at our employees file. We've extracted some sample data to make it easy for us to work with. So we just take a look at this sample data. So we've just extracted a few rows from the employees file on we can see here that Yeah, Bob does seem tohave a correct SSN where, as we look at Jane and she doesn't and the game we go down Thio characters like Michael and that s SN doesn't seem to be correct. Brad and Jack also seem to be missing a correct SSN little over how we can extract those records that have then a correct or incorrect SSN really, whichever way we want to do it. So if we go through and look for grip, we want to look at the records that don't have correct entry. Then we're gonna use minus V to reverse the search. Because, of course, it's gonna be a lot easier to search for the S S N so we can go through and use our enhance regular expression because we're gonna be using those quantifies within braces. Open up, then our single quotes on we could say that. Okay, we want a word boundary. She's going to include things like the comma and then we can go through and say that we need a digit so zero through to nine on we could specify how many digits do you want? We want three digits for the first part of the SS in, so we can say there. We require three digits. We're then going to have a hyphen. So the hyphen is just plain text that we're putting in that again. We're going to look for some more digits now we need to dish. It's so we can go zero through to nine. And then again, we just go through, say that we require two of those. How easy could this be again? We want then just a clear text hyphen. And then we require four numbers, so zero through to nine clothes off our brackets. Open up. Our brace put in the number four were really rocking some here, and then we could put a word boundary. Or we could even say it's gonna be the end off the line. But in case you got some odd spaces and things like that, and then then we'll put our word boundary being careful not to put the Capital B in because, of course, that Capital B is looking for a non word boundary. We're going to extract this then from the employees record. And sure enough, we got them. Brad, Michael, Jack and Jane. If we go through want to look at those with a correct employee record, then we removed the reversing off it. So we see Bob, Sally and you, Anna. So now we have those users who have correct records. And of course, if we need this to persist, it's very easy just to redirect the output either of the incorrect ones or the correct ones through to a file. But you'll see later when we start looking at, said said, is really designed to be editing a file so we could edit the file to remove it. But most likely will be writing to another file those users who had incorrect entries so we can work on those two. Make sure somebody follows up without uses to get a correct Social Security number. That's just amazing. We've built another application, and Danny and D I Y R us are doing really well, so let's jump back into the slides just to see how well they're really doing
Summary

[Autogenerated] now where I really do hope to becoming perfectly clear to you is the importance of being able to build this knowledge by extracting data from your files for D. I. Y. R us. They've already shown an improved performance. And how do you think you might be able to improve the performance of your organization by being able to quickly and effectively retrieve information? During this section? We've looked at regular expressions on we've seen How weaken right thes regular expressions to help extract the data. To build our knowledge, we've used things like anchors of a dollar symbol representing the end of the line on the carrot symbol representing the beginning of the line for the line that could also be a standalone string that we're searching. We've looked at some of our boundary characters, so the word boundary, which can include things like the hyphen on we're looking at white space, including things like the tab in the space characters. Then we've looked at our quantify ears, and this is where we start thinking this is just reading a Batman comic or something like this. But we've seen that we can use things like the brace brackets and put an absolute number in there. We could use something like the question mark where we're really making the previous character optional. But we are having a character we can only have one of wth, Um, whereas we go through a look at him like the star were still making the previous character optional. However, we could have any amount of those characters within a consecutive string and then the plus. And that's saying where we can have one or more of the previous characters. So we've seen how we can use thes quantifies to enhance our regular expressions. We ended up this module by looking at implementing validation of HR records and using them. The skills that we've built up were ableto help. Then build this application to search our HR records for Social Security numbers. Next up, we're going to make our start on our journey with, said the Stream editor. So please join me for this module
Fundamentals of sed
Introducing sed

[Autogenerated] Hello. My name is Andrew. We're gonna be taking a look now at the fundamentals of said within this module, we need to do so because Danny has started using said to remotely edit configuration files over sshh. So we need to keep up with me, and we're gonna learn how to use said to reduce our administration time both on local service and remote servers. In doing so, we're going to become familiar with said, as is Danny. And we got to start using the print command, the substitute command, the upend insert, delete commands. We'll learn how to compile and write statements that allow for multiple expressions and use. Said files we have course will implement in place edits of files rather than just sending two standard output. And all of this will culminate in remote edits over. Sshh. Really reduce your administration times to begin with. We can look at this, said print command. We can use something like this where was just said and P for print, and then we're here. We're looking at the TC password command. The P is really printing the pattern space. The matched line as we haven't put anything for arranging will match every line. The downside of this is that we'll actually get both lines printing out. So if we've got 10 lines in the password file, we're gonna get 20 lines. So the pattern space, as well as three ordinary standard output to suppress the standard output we can use the minus n option to set This could be really useful, because if we working with a large file, if we want to make some configuration changes, we want to check those changes. And it's going to be a lot easier just to see the lines that have matched our criteria rather than seeing every light. We also could include ranges, and here we put a couple of ways that we can do it. The first range is looking at lines one through 23 and we're going to print those. And, of course, with suppressing the standard output. So is only those matched lines, and in the second example, we using then a pattern on that pattern, then is delimited by the forward slashes. We're saying that lines beginning with root. Of course, we print out those lines. There's all well and good seeing in the slides, but we're gonna become most familiar with, said, by working on the command line, off are put into desktop. So let's jump to the command line straight away and see how said goes together.
Print Command

[Autogenerated] So we've installed ourselves back on the command line off the U Bahn to 14.0, for desktop. We'll start taking a look at said So to begin with again. If we just start off with something quite simple, we go through and print out. Then let's say just our test file on, we could see we get the output being printed out. But each line is duplicated and working with a small fire like this. It's quite easy to see what's happening. Let's clear the screen on, then include the minus end Option two. You said that will suppress standard output. This is most useful, especially when we're doing a substitute so we could see that the lines have worked properly because you could imagine if we were replacing, Let's say the word end with the word finish or something like this. With this was many hundreds of lines. We're not necessarily going to be easy able to see our result, but by suppressing the results and only showing the matched lines. Then, of course it becomes a lot easier. But also, when we want to start including ranges within our example, it's okay, Let's go back clear their screen and now go through, start putting, arranging. We can put our ranging again. They're going to be with inside the single quote, but we could have something simple, like one through 25 Now, when we go through and look at this, we've got simply lines one through 25 being printed out. We could also do something like this where we could say that we won't line five through to the end off the document. So without having to know the last line, we just know we want to start on Line five and continue until the end of the document so we could see that we're starting then on the B line and going through all the way through to the extra postcode inflation that we put in in earlier exercises. As well as doing this, we can go through and put in regular expressions When we put in regular expressions, those regular expressions are going to be delimited within the range using to forward slashes. So within the forward slashes, we specify what we want. We might say, Look, we wanted to start within the letter A. So if we go through and look at this. Now we could see we're getting that coming through the course. We could start a pending this to then say that we want, then a followed by zero through nine on, Let's say, Just run on that. So we've got now a 234 showing Yes, the A two that is matching rather than the a bee that we saw before so again we could start building this up. But this is just giving you a little bit of an idea of how we can use the print command. We can also use the print command with some of the other commands such a substitute, so we can actually print out those matching lines. Let's jump back now to our slides and look at some more off the commands that we can use with, said.
Introducing the Substitute Feature Within sed

[Autogenerated] if you haven't come across it already, you're soon gonna learn that substitute is just one of those really major commands within. Said the sub Superman really is the search and replace tool. So if we want to go through, replaced one word with another, then that's where our substitute command is going to come in. It could be written like this or or something similar to this where we have the option range that we want to work with. And then we have the substitute commands of s and then the character immediately following the s is often a forward slash, but it really represents the DL emitter the we're going to use to delimit their string on the replacement string. The string is really the word that we're searching for. On the replacement is what we want to replace it with. We look an example off this. We're working only on lines that begin with Gretchen on. We're searching for forward slash been forward slash bash on. We're replacing it with forward slash Been forward slash s. But you can see the DL emitters that we're using for substitute Have bean replaced with the act symbol. We have to be consistent with these delimit er's. But we choose the DL emitter that we would like to use being the first character after the s. Now, the idea of using the app symbol here is if we use the normal forward slash, then we would have to escape the full would slash for our path. So it really makes sense choosing a different delimit er. So of course, the ultimate upshot of what we're doing here is we're replacing the default shell from bash to just the born shell for the user's Gretchen. It has to be noted and will remind ourselves of this later on. That this is only printing to standard output were no implementing a change. Very often we would run this and probably include the P Command on the end and we'll see this in the demonstrations and suppress our standard output. So we only see the matched line, the pattern space. This way we can check that the edit is going to work and once we're happy that the editor is going to work, we can include the minus I option to ensure the file is edited. But for the moment, we just want to get used to the search and replace. So enough of these slides were going to get into the command line and just adding in the little postscript here that we do need to replace the option Maur than once per line than that's where we can use the G or global option. It will replace it if we don't include G for as many lines that we match. But it only replaces the first occurrence for each line. Using G will replace it globally throughout the file, but enough of this to the command line we go.
Using the Substitute Feature Within sed

[Autogenerated] so more command line fun than working at the abortion to $14. 04 desktop. But remember, it really shouldn't matter the version or distribution of Linux that you are using, as well as the fact that you could be using Mac or other UNIX systems. So we're gonna be looking at the substitute command now. So if we go through and just look at some simple idea that should be able to give you some practical advice straight away, if I go through and take a look at my past C S V script, we look at the script of the moment. It's not tabbed in. So even though it's only a small scripts was not a major problem. I would prefer the code within the dew and done block to be tabbed in to make it more easily read. Now, of course, just with the cat command. We're not getting our line numbering, so it's hard to see the range. But we have got a command called En El, and if we go through and take a look at this within their pars, Cioffi, we get the line numbering. So when I'm looking, it is really lying six through to nine. That becomes my range, and it's those that I want to embed or in dent a little bit. Now in itself. Some people doesn't think of this as a search and replace, but I'm really searching and replacing the beginning off the line with a few extra spaces. Celeste, see how said could do this for us. If I use said on, we're going Thio, open up their single quotes on We're going to start off by putting in, then the range that we want to work for here is gonna be a new miracle range. Suggest lying numbering. So it's six through turn nine. We want to substitute and we can use the standard deal emitters of the forward slash here. So we want to look for the beginning off the line, and we're going to replace the beginning off the line with We can have the 1234 spaces would be fairly standard for an in debt. I don't need to worry about doing this globally because the start of the line is only going to occur want, but there's no harm, including the G option here. Once we're finished with this I close out of my quotes and then we specify what we want to work with this. You're gonna be there, My past C s V and there will we Look, everything is nicely indented. We're seeing the complete file here. But if I wanted to only show the match lines especially useful when it's just text that we're replacing that I can use minus end to suppress the standard output on, we can include the ___ on the end here. So I'm just gonna replace the key with P to print out my pattern space. So now we just see the lines that we've indented. No, we haven't actually made any changes. We would have to redirect the output through to another file for it to change. But we'll see later on that will finish off this by looking at the minus. I option. We've reviewed the other edits that we can do. We should include the minus I option, in which case were then, using an inner place edit. But straightaway we could see how powerful this becomes being able to in Dent and tidy up our scripts. If we're looking at making other changes, let's look at what we were looking at within the slights of the example. It's okay. Let's suppress our standard output and we'll go through on Look at a pattern. The pattern always has to be forward slashes, and we're gonna look for then lines that begin with, say, Gretchen Grete probably is going to be enough, and then we're going to substitute. Of course, we want to look at a path here, so the path is going to use the forward slashes. So it's a good idea to use some other character so you can really choose the character that you want to use, so long as it's not going to appear anywhere else in your strings. So the axiom will seems quite useful for me, and then I can go through and say We want to replace slash been slash bash. We want to replace it with Slash been such S H and then we finish off our limiters look of it have already messed it up because I'm used to it. But of course we using the act symbol so you can see the idea of using the at simple because I had the double forward slashes in here. We would have got confused or the system. What about confused where the DL emitters were? So of course, I deliberately put in the incorrect characters they're just to highlight. My point again will suppress our standard output and we'll close off our quotes, which will just do there on. Then we'll go through and look at the E T. C Fash password file. Once we're happy with this and again we can hit. Enter on, we can see there. We've got the Gretchen line and we can see there. We've got the default shell set for her. The last element there slash been slash. Shh. We could see the usefulness off the minus end, because if I don't include the minus end and let's take out the pea. Otherwise we'll get the Gretchen line printed twice. Now when we go through and look at it, of course, we've got to search the output for Gretchen. It's not so bad because she's one of the last uses that were created and she shows towards the bottom of the file. But you could imagine where we have lots of output here. It's not too certain whether or not we're making these edits correctly. Let's clear out the screen. Let's move back now to our slice so we could start gaining some more knowledge of what else is possible With said, we'll start looking at the insert the upend and delete statement.
Insert, Append, and Delete

[Autogenerated] Now, I am rather hoping that you're not gonna be surprised to find out that we're not limited just to the substitute command. When it comes to writing to a file we could also use in the imp end insert and delete commands. Here we're we're looking, at example, off using the A pen command. We're looking for the line that begins with Server Space three, and we're a pending Zehr adding a line afterward, and it's going to be then server space. NTP dot example dot com on all of this, then is happening with the TC NTP dot com so you can see the upend here. We're including the pattern range to search for and upend after that line. Similarly, we could insert. So here we're looking for the line that begins server space zero on above that line. We're going to insert a new line the same textures before server, NTP dot example dot com. And again, we're working with that same file the NTP dot com When we're looking at the delete. Come on, the delete commanders. The name kind of suggests, I suppose, is deleting a line here. We're looking for lines that begin with server and a space followed by a single number. And then we have to escape. The dot the dot has a special meaning, meaning a single character within the regular expression. So escape it. So we reading that as literally a dot and then or Bunty we delete those lines. I think there should be about three lines that match. I think its servers one through or zero through 23 Actually, there will be four lines again reminding ourselves that we're not using the minus I option. So we're not actually making any changes. But again, it's a good idea toe print to standard output. First, just to verify everything is working the way you intended. Once we're happy with it and we've developed something that could work. Of course, we can use the minus I. We could work with local files. And then, of course, we can implement the remote it, knowing that our file or the script that we've created is going to be accurate and is going to behave the same across any NTP topped comfort. But enough of this yammering. Let's get straight into our command line and have a look at these inserts a pens on. Deletes
Demo: Using Inserts, Appends, and Deletions

[Autogenerated] So a comfortably back now working on the command line of our upon to server on, we're going to look at how we can use the upend in certain delete commands with said so There again, we could look at ways to edit it. When we look at our file the NTP dot com, we could start seeing that we've got entries here for various servers, but we might want to replace the existing server entries. So as we look at the file, we could see that we've got Server Zero through to Server three as well a server. NTP door up 12 dot com so we can look at ways that we can insert data, always weaken, delete data or append data into this configuration file. Of course, once we have the configuration correct, then it becomes very easily to be able to run this across multiple service. So taking a look at how we might put some data, it will work with upend. First on, we can then use the command said, put in what we're looking for. So we open up our quotes and then say that we're going to look for the line that begins with. We're gonna use the carrot symbol server on we can put in the space character on. We'll look them for service space three. We're gonna use then the pen command. So we just need the letter A for our command. Because either all of these commands just single characters on were a pending then server space, NTP dot example dot com or why ever time server we'd like to put in Once we're happy with that, we close off our quote and then, of course, put the file name that we want to be working with. Remember, we work with, ah, file or input. Now we can see quite simply following Server three, we've got our own new entry that we've upended afterward. Server into p dot example dot com Again, we see how really simple these edits are to make. Of course, we're not editing the file at the moment, but it's always a good idea to run to standard output. First, just to check now, returning there, we can go through and just change this to eye for insert. So instead of putting the line afterwards, the line now is going to come before and sure enough, when we look at the output, we've got server NTP dor example dot com that then immediately is proceeding the server three but as well as thes inserts and upend. And sometimes we're gonna want to both insert on delete. But we're gonna need to know the delete command as well. So is working very similarly to this. We put in the range that we want to delete. Otherwise, we're gonna be deleting everything so we can use that server. And then just as a reminder, we can use these boundaries so we could say server followed by a space boundary. So any white space character, but only one of them and then only one digit zero through nine, we're going to use the escape character. So they're backslash because the dot is a special character for regular expressions. It represents a single character. So we just say that we want that literally adopt and then followed by up 12 That's probably although we need for the domain name. And then, of course, we don't need to worry about our inserted text. We just changed them to a D for delete. It doesn't really matter about the extra white space on, we could see that we've magically be able to delete Server Zero through to Server three. So how simple is this'll? CE? But I say very often we need to combine these commands or said expressions together so conveniently. When we start looking at the next set of slides will be introducing multi line statements or expressions was said, and how we can utilize them with said control files. We don't need to use the set files. What becomes several ways that we can implement multi line expressions. But of course, a set file becomes repeatedly correct that we can use. Let's jump back now to our slide to look at how we can combine, then inserts and deletes together toe finalize our application.
Using Multiple Expressions Within sed

[Autogenerated] So within the previous clip, you saw the importance and perhaps the need where we might want to express multiple, said commands together as one block. We can do this in a number of ways. One such way would be to use the command line and using our brace brackets. So opening up our brace brackets, we can then simply hit return, and we're going to be giving a continuation prompt on. Then we can go through and insert our new example and then continue to delete our old examples. We close off the brace brackets, clothes off our quotes and then make the change to our file. But again, without the minus I option. We're not actually making any permanent change if we want to make the change a little bit more easier to deal with than we can create NTP dot said file. Or in this case, we're calling NTP DOT said, the name is only for guidance for us. NTP, meaning it's working with the NTP file DOT said, indicating to us that Issa said, control file on, then we simply have in this case, these two lines the insert on the deletes when we implement it we can use, then said to minus lower case F the name of the control file and, of course, under file that we want to work with again without the minus. I option was still not implementing any change, but it becomes a very easy way that we can test. Then the file is working nicely for us. Once we've verified that control file that were quite happy than to implement the minus I option. It's also then very easy for code reuse now that we've got the control file because we're able to into copy this up to remote servers and implement those same changes across our remote service. We're not writing anything until we include that minus I option on. Don't forget, we can use minus I than with an extension like dot back to make sure we back up that file prior to the edit. So let's look at this and multiple expressions by moving out to the command line of are a boon to desktop so, as we've seen very often weaken need to combine multiple changes together. So this is where they're multiple expressions within, said they're going to be useful. I just copied the original backup file back to the NTP dot com. So if we look at the original NTP dot com Fidel, when we look at it, we've got it back. Everything as was before. So we've got all of the spaces and the commented lines. And, of course, this is the way perhaps our existing servers are going to appear or certainly new installations on up 12 with the MTP server. So if we want to look at how we could implement some of these changes, we could do it from the command line using set on. Let's just stick with what we've been working with so far, so we just stick it, replacing some of the entries. But when we go through and put our quotes, then we can then go through an open up our brace brackets. We can specify then that perhaps we want to search for lines that begin with server, and then we might have say, serve ah zero. And we might want to insert after this so we could insert our entry, then server and then NTP dot example dot com, or what else that we want. That's all that we really need to worry about. We don't need to worry about anything else within this. But we want to also go through and delete those lines that begin with server, followed by some form of space, followed by then a single digit, and I won't worry about the rest of it. For the moment, this should be enough clothes off that regular expression with the forward slash. And then, of course, we want to delete those we can close off. Then the multi line expression. Close off our quote and then, of course, and go through and put the mtp dot com file that we want to work with. So we put that in and then we go back and scroll up and have a look at our output. We can see there that we've got our NTP dot example the NTP dob 12 dot com But their server zero, the server one et cetera, have gone. So that shows how we can do it because we got a type everything in if we take a look at using some form of control file and here I've already created the mtp dot said ready Got four expressions hit. So we've got the deletion off the empty lines. We then got the delish in of the commented lines and we're including Then the slash s at the beginning, we say we can have a space or no space. So it will begin with a comment, with or without spaces in front of it. Again, we're gonna delete those lines we could see there were inserting, then above the service zero line our server entry. We're putting the prefer flag next to that on we're deleting then their server zero through to serve our four entries that relate back to Dublin to dot or GE to see how we can implement this. And it's going to be so simple from the command line now because it's simply going to be said minus f for the control file that NTP dot said and then reference. Then our file and tp dot com Again, we're not making any changes, but we can check then our change on the screen. This now seems perfect for what we want on. We're able now toe implement our change, so we go minus I on. We could also then specify that we want to create then a file called dot back. Of course, we don't have permissions, a standard users, so it's proving that we can read the file, and that's all that we're doing with, Normal said. When it comes to writing it, of course, we're gonna need permissions. Let's just go back and then put in the US a. Do command in front of this. So so do now the change has taken place. And how simple is this? So now when we go through and take a look at the MTV dot com, we got then the change and working file that we want only being able to it so quickly and so easily, and when we look at it, we've also got then the backup file, which is the original file. So we're now ready, really? To start looking at how we can implement this across multiple servers. We've seen that we can implement the change locally. We're happy with this now. We need to implement this across multiple service
Remote Edits Using sed Over SSH

[Autogenerated] as we approached the scenario for this module and started get quite excited because I just know how useful this is going to be as we look at how we can remotely edit files using SS H and emulating what we've seen Danny being able to do using the SS H client on losers, we can use Shue minus t. Let's go into a seiner, sir. T t y allowing us to interview versa. Do password. The minus. I extension that we see here when we run this to do and said command is in allowing us to create the backup file. And when we referenced with minus f, the remote file NTP dot said must be on the remote server. But in this way, we can effectively edit files across multiple servers easily, effectively and reliably correct. So we just need to get in and see this. Now here we have a second up onto machine. I just changed the background color so we can see it. We could see the I P address is 105 Where we take a look at the output from I p address show. When we take a look at the e T c on DDE n tp dot com We could see it's pretty much the standard NTP dot com Let's return then back to our main client that we have been working with. We're going to start toe implement the change for this now for using the SS H client. Here we continues SCP on. We're going to copy across the NTP dot set so we can use then NTP dot said we want to copy this across as ah user at 192 Don't 168 0.0 dot 105 We're gonna copy it across to the temp directory. So this becomes in our control file. We can accept the fingerprint on put in the password, ideally, would already have their uses key shared with this, so it becomes easier in authenticating. So now that we have the control file, we're going to use then said toe edit this S s h minus t. And then again, we're going to use user at 192.168 dot 0.5. We need the minus t option because, of course, we need to be able to put in the password to the City Command. So we're gonna run then So do and said minus I, doc back to well, to create the backup file minus f we're referencing the file on the remote server. So the NTP dot said we want to work upon again the e t cedar into be dot com on the remote machine. Once we're ready with this, we can Then here, enter on. We're putting in the SS H password, and now we're being prompted through the t t y. That's being assigned to putting the password for the user. Now, it might appear to this that our connection just being closed, but it has because we finished running the command. But our edit now on the remote server is reliably correct. To be honest, I don't even need to check it. I know it's going to be in working. However, I could understand that you might be a little happier if we go through and actually check this. Well, that's that. Just jump across back onto that machine, and now will we go through on Look at the NTP dot com. It's being edited correctly, and of course, we're gonna have the backup file that will have been created as well. So this really is one of those moments you could start seeing. This is really important, and I am going to be able to implement this at my work. But certainly I do hope you have found this useful and you will implement it.
Summary

[Autogenerated] so I think this has really been quite an achievement within this section, we've been able to see some really powerful tools. I do hope you've been able to use them. Danny has been able to share some of his knowledge and show us how he can edit files, including maintaining this remotely virus. Shh. We've learned some basic set techniques on when we've gone through. We've gone through. We started off with just the P for print, but we've also gone through looking at our inserts. Deletes a pens, et cetera. We've seen how we can use multiple expressions either using the brace bracket. That's great. Where there once off from the command line, but very often where we need this to be repeatable, we're going to create, said control files on. We can include those with the minus F option. And we've seen that we've been able to remotely implement these changes using SS H. But if we're going to use to do virus Shh, don't forget to include then the minus T option so that we're sign a t t y to the connection, enabling us to type in US Ado password. So next up, if you're ready, we'll start looking at substitution grouping using, said
Substitution Grouping with sed
Introducing Substitution Groups

[Autogenerated] welcome to this plural site presentation. My name is Andrew Mallet, and we're going to take a look in this presentation at Substitution Grouping with said and then he's going to need to stretch himself for he needs to be able to transform last names through toe uppercase having these two group digits for readability. The solution that Danny's gonna find is to use said substitution grouping. Now, what do we mean by this? We're going to start then our substitution on dhe, then this. It looks fairly ugly, but we're grouping is defined with our escaped parentheses. So we use the backslash to escape the round bracket. And then we're going in this group to say that we're looking then for characters that are not commerce. So we really defining our field separator is being a comma after that, then ranged that we're looking for. We could see that we're looking in for zero or any amount of characters, and then we end the grouping. So each group is going to be defined around the comma for a replacement string we're going to use than the upper case command on. We're goingto upper case, the first grouping. So it looks fairly unpalatable. But again, it's just a matter of breaking this down and looking at it bit by bit. It'll piece by piece. The personnel department has asked Danny to take a look at the employees file. We need to upper case the thirst field on that represents the last name on this is how we would achieve it. You can see within the said string that we using the S for substitution and then just for ease of readability. We're using the delimit er as the act symbol. We've got our first grouping defined. Remember we using the escape character or are back slash and then the parentheses on we say it is a comma separated file. Then the elements that make up the group are everything that isn't than the comma. We look at the first field on we upper cases, and this becomes the replacement strength working with in this case, the employees file we can see highlighted here. The surge string becomes the definition for the field on the replacement string, then becomes the commander upper case. The first field, one indicating our first field. We only defined one grouping, so we only have one field that's being defined when we extend this a little bit to look at what we need to achieve by upper casing the surname on by lower casing the first name. Then we get our two fields so we could see within our substitution strength. We go through and define to field in these two fields, our commerce separated. So even though our employees violence a C s fi file, we define that within then our round brackets. But each round bracket is separated by a calmer itself. So this is just what we use within our substitution string. When we look at our replacement string, we could see that we're up a casing field one on lower casing field, too. So we receive then the desired result. Through this rather uncomfortable looking arrangements of characters, we really need a little bit of time to be able to assimilate this. So let's spend a little bit time now working on Arab one to command line to see this in action
Formatting User Names

[Autogenerated] So welcome back to the command line, then of arable into desktop, and I hope you're ready to help the personnel department. They've requested a CZ we've seen within the previous slide that we work with the casing off the employee's name within the files. Let's just take a look at that file. We go through and take a look at the employees file. We could see that we've got mixed case, last names and first names. So we want to make sure that the last name Joe's Jackson Federer, et cetera, is in upper case on, then the first name Bob Jane, Jack et cetera. We want to ensure that that's entirely within lower case. So to be able to do this, we've seen that we can use substitution groups with said so. Although they're not the most beautiful off strings, we're going to see how they're written. So if we go back into our command line and open up, said will work. First of all, just with one substitution group on, we'll look at upper casing the last name, so we're going to use then our substitute string. We can open up. We don't need to use the accent, but we can just use the forward slash because we're not duplicating this. It will look a little bit weird because, of course, we go straight away to escape the parentheses all round bracket. But we don't have to worry about the duplication of the forward slashes off the DL emitter that we're using so we can just use the Fort Worth station. I'm going to stick with the forward slash At the moment on. We're saying that the first grouping is maintained around anything that's not a comma on. We can have zero or any amount of characters before the comma. We close off the substitution group again. Let's with the parentheses. But we escaped the parentheses. With that done, we then say That's my substitution string over and done with, and we're now into the replacement string on. We're going to say that we're going to replace it by upper casing the first substitution group. So number one, this becomes that a replacement string. And of course, we end the replacement string. We close off our quotes and we work with our lovely employees file. And now when we hit the enter key, we've magically upper case that last name. So we know then that our personnel department are going to be pleased with us or release when we managed to do something similar by lower casing the first name. So let's go and clear the screen and let's get back to a little bit of space. Are Parro so we could see what's going on here? So this is our first substitution group. So still working within our substitution string, we put in a comma two separate thes substitution groups on we create a new one. To be honest, it's going to read exactly the same as the first. So we say Yeah, anything that is not a coma. We close that off clothes off, then the substitution Shin group. But before that we must have to say we can have any amount of characters close off the substitution group, so that becomes then our substitution Group number two on we could see one and to adjust themselves comma separated. We need to go to our replacement string and within the replacement string. Then we're going to lower case substitution group too close out our substitution or our replacement string rather on dhe, then hit the enter key and you can see now we got everything the way we would like. We can see, though, that we're losing the comma in our output and that's okay. We can add it back in just by adding a comma after the one So we can see now that we've got everything the way we would like and we've got, Then Output is going to make our personnel department happy, reminding ourselves that we haven't actually made the change here. To make the change, of course, would have to send the output to another file will use the minus I option. But for the moment, we've seen this working, and that's all that we really need to be able to do. Let's move back now to the slides to see what else needs to be done with these substitution groups on, I think you'll find that we're gonna need to format some output for the sales department
Understanding Numerical Grouping

[Autogenerated] Well, I must say that so far from what the Personnel Department have seen, they think that Danny is simply awesome. And you, too, could be as Danny is you and said together make a great team. We're going to continue the thing, though, but perhaps feed you something just a little bit more challenging. How does this grab you? When we look at the output, we're grouping numbers into 1000. So we really adding in the Kama in front of the last three numbers on we're working with is you could see the file here forward slash Prock forward slash load average This way, hopefully the number becomes a little bit more readable, being able to see what makes up 1000 in the output when we look at the replacement string where printing the first group, the second group on the third group with a comma in front of the third group were including the G option. So we can do this globally in case there's Maur than one occurrence of this per line. Ultimately, what we're looking for is for a group of three numbers and then that becomes our third group that we print. We've also had something similar being requested by the sales team. So we need to be ableto edit the catalogs on A LL catalog files within the catalog directory, replacing the field DL emitters with Poland's. And we're gonna need to do that, because if we're going to start com, arise the numbers if you like, we want to make sure that we're not going to get confused with our field DL emitters. So first of all, we're gonna replace the commerce with Poland. And then we're going to start grouping the price column into groups of one thousands to do that, then we got to create a catalogue dot said file. So the first element is their substitute in replace where we're looking at replacing the comma with the Cobra colon globally across the whole line on. Then we go through and look at creating than the price column to be including the Kama around the 1000. We don't want to do this just with one file. So we working within their catalog file and all catalogs within that catalog directory. Let's take a look. This is how we comfortable work with our load average file. Once we've had a little bit of practice with that we can move into then the task that the sales department would like us to work on with their catalogs.
Updating the Product Catalogs

[Autogenerated] So we're back now working on the command line over the open to desktop and let's take a look at the load average. Five. I cat out the four slash Prock forward slash load average file. We can see the load average over the last 5 10 and 15 minutes. We could see the number of tasks that running. So we've got one out of 338 running, and then we've got the next process idea that's going to be used. 7567. It's not too unreadable because it's only the four digits, Of course, is that number becomes ever increasing. It's going to be more convenient if we read it with Kongers around the 1000 point. So weakened. Do this using set. If I take a look, I've already created the catalog dot said file, But the catalog dot said Falcon really work with anything that we want calm, arise like this. So if I go through and take a look at the catalog dot said file and we can see within the first line that we're gonna replace commerce with Coghlan's well, that doesn't really matter, because I don't have any in the load average file and then, as we saw within the slides that were looking then to put commerce around our groups of 1000. So it's a simple solution, although it might read a little untidy early. If we go through and take a look at this. Of course, this is a real time file, so we would never be editing. The file is such in any case, but I can use then said and then minus F refer to my catalog files of the catalogue dot said file. And then the load average files. Oh forward slash Prock forward slash load average on. We could see now that we've got 7584 and it's just more convenient to read in this format if we take a look at the catalog directory. So if I go through and catalog and star about a couple of sample files in here and we could see that we've got then the fork we could see that's costing 50. Andi, we've got 21 of those as we go on down, then we've got a ride on mower, and that's costing 3199 and we've got three of those in stock. So we've got a couple of files within their that We're looking at the tools and the garden catalog, but we can then magically make changes to these. First of all, let's double check by sea. The output is what we would like So we can go said minus f And then our catalogue dot said on we're gonna have this working then on A LL files that we find within the catalog directory. So again we could see now this is formatted Maur the way that we want on We could see that we have changed it from comma separated files to colon separated files. So now then the calmer that we add in for our price column doesn't affect the schemer off our files. Once we're happy with that help, let's go through and implement the change. We won't worry with a backup, and now will we go through into that catalog directory. Let's go through clear it. We can go through. We've got the two files. Let's just say cat out the garden filed to begin with so we can see that's formatted correctly and then taking a look then, at our tools file on, we could see that both these files are now formatted the way the sales department would want. And again, we've seen how easy it can be again. The complexity of calm arose in the number doesn't really matter so much because you could see once we've implemented it wants than we can carry forward that implementation through the said file. So let's move back into our slide now and summarize what we've looked at with our said and substitution grouping.
Summary

[Autogenerated] So were the end of another section on. We see within this section that Danny has been able to keep both the sales and the HR department happy. Working with the HR department, we've looked at formatting the employees file. So the weekend translate the whole of the surname through toe upper case on working with the first name were able to translate that hole of the first name through to lower case the sales department needed us to do something similar. So working with said substitution groups, we were able to tidy up the display of the price column within the catalogs, making sure that we commerce separate the thousands. The difficulty here was that it was a comma separated file, so we needed to replace our commerce with Poland's so that we were able to use the comma as our separator within our thousands. To make this possible, we've been using the said substitution groups on Remember. That's where we use the escape character in front of our opening and closing parental thes. So this has bean, the tool that we've used to create, said substitution groups. So hopefully you found this youthful, you're gonna come up to see how we can next, you said toe execute command
Executing Commands with sed
Introduction

[Autogenerated] I am quite sure that there are many inexperienced limits. Administrator out there who does know actually realize they're executing commands with said is possible. My name is Andrew Mallet, and welcome to this plural site presentation. We're we're gonna show you how you can execute external commands using your good friend said Danny's being required to archive old catalogs so the sales department has never quite satisfied was what he's already achieved for them. So the old D. I. Y. R s catalogs are going to be listed in a text file. The process is currently that Danny reads through these text files and manually archives those catalogues. However, Danny really needs to automate this. He's finding that he'd like to spend a little bit more time on the golf course and a little bit less time within the server room. Seeing how this could be achieved, we could see here that we're using the said command on replacing the beginning of the line with the command. L s minus. L on. Don't forget the space before the end off the replacement string. Using the execute command, we can then interrogate the list of files and insert the L s minor cell in front of each one of those lines to see how we're going to use it. Let's look at how with my archive these catalogs, When we look at our string, it seems a little bit more, but we've included a range operator. The range operator here is requiring that the line in the catalog dot list starts with a forward slash. We can't change the deal emitters within this range operator, we have to use forward slashes, which means that we have to escape the forward slash for the path within our replacement string where we're looking at substituting the beginning of the line, we're appending to a tower file. Are we using tar minus R and then, of course, F for the name of the Tar File, the Tar File. The catalog doctor must already exist, so we can only upend with this. The list of archives are then going to be in the file cat dot list. Once we've done this, we can then go through and execute the R M command against those catalog files. So again the said command becomes very similar. But in this case, we're using the R M command to remove the old catalog. With the understanding now in place, let's move through to the above into machine and actually see this in process.
Executing Command with sed

[Autogenerated] So we're going to see now just how powerful, said congee Within your own organisation says, Just look at what we can do Let's take a look at a file that we have here. So if I cat out file dot list, we could see we've got a couple of file names in their E T C hosts and E T. C service is So it's a demonstration. We could see some of the things that we can do with said and executes if I use said, we can go through then and specify if we wanted to arrange. But because everything matches what we require there. There's no real need for us to put arranging. So I'm just gonna have my substitute command straight away, and we're going to look at substituting the beginning off the string on. We're going to substitute it. Let's say with something simple, like unless minus l put in this space because, of course, we want space out this between the command on. Then we're going to use the E for execute. We read in, then our file list, and then as we go through and execute that, we could see that we get a long listing of both of those files. I can clear the screen. Go back up if we want to run another command. Unless look at the stat command against those we could see then that we get the full statistics listing of both easy see hosts on DTH E E T. C. Service is. But of course, we're not limited. Just tow working with files. Let's clear this out. And again. Let's go through and cat out. They use a dot list here. We've just got to potential user names. Gwen and Monty Celeste. You see what we could perhaps manipulate with this? We can then go through and use our said command. Open up, put in our substitute again. We were working with the beginning of the line and replacing the beginning of the line, this time within. So do and user ad put in our space. And that's the end of our replace string. We using then the executed as we were before, and it's matter of simply putting in now our user list file. Run through and do this, and we've created those two users. When we look at tale than minus end to look at our E T C password file. We could see that. Sure enough, we've got Gwen and Monty, but maybe we don't actually want those users so we can go back up. And then instead of user ad, we can use that same list and go through and put our user del minus our command so we could see now that we've gone through and removed the users and check for their home directory on dhe school. File on all of that, then has been correctly removed, as we have with our user dot list. So there's a lot that we can start to work with. But as we've seen that, we want to go through and take a look, then at archiving are catalogs. So if we take a look at, then the cat dot list file on, we've got the full path here through so forward slash home forward slash user forward slash catalog on. We're removing the Garden catalog, so we want to implement this. Obviously, we're just looking at a sample here, but as our list then arrives or we need to do is run, said command, we can double check that. We've got a full file system path that's being included so we can put in our range. Operator were saying then that the regular expression that we're looking for the range of rose to work with will begin with then a forward slash. But we have to escape that forward slash because it's within. Then our pattern for the regular expression we closed. Then the pattern range on. We go into our substitute command again, same as before. We go through to say that we want to then go through and look for lines beginning with on. We replace that, then the beginning of the line with our tar minus our command. We specify the F the miners are then becomes the upend. The F becomes the name of the file that we want to write to. So catalog doctor on that file must exist to be able to append to it and then slash e we could think I threw end out the said escape sequence. We can work with our cat files our cat dot list and now we've added it through to our tire file. When we go through a look at that tower file Now, Toby, go through do Atar minus t put it in the file name and then our catalogue dot Huh? So we could see there that we have been the garden fire. We have the numbers file we have on NTP dot said file there. So we've got a few items in there already, but we can see now that we've been able to correctly archive the garden fire, it's clear their cell. But of course, we're not finished there with our garden file. We need to go through and removed that now so we can go back to our said sequence and then it's just going to be a matter off r M. And then, of course, we can force it if we want. So we want to remove whatever we have within our catalogue list. Once we're happy with our sequence, we can hear our Internet. And now when we go through and less out their catalog directory, well, you see, we just have the one file in There are tools catalog. So in this way, of course, we could build this into a said file, so at least it thing controlled and we know we're doing everything correctly. We could even build it into a bash script. But what we're looking at is reliably and repeatedly being able to run the tasks that being asked from us. So we've seen that we've been able to work with Fei also listing out our files starting out our files. We've moved on to see that we can create users and weaken delete users based on a list so really useful again what we're getting information about from the personnel department, about new starters or about levers within our organization. But then, for the specific application to enable a little bit more time on the golf course for Danny, we've been able to go through on dhe. Implement on Automation will weaken ad, then the correct catalogs to the archive and delete the catalog from the catalogue directory. So of course, now let's move back into the slides, and we can actually start looking at how we can execute some of these said statements from Inside of our Thigh or Vim text Editor
Using sed Within vim

[Autogenerated] so we're gradually reaching the end of our section On, said on. We see now that we can use said to run external commands. Let's turn this on the head Let's see that we can run said from inside Off our text Editors using Fi or the Vim text editor looking example here, we can see that we can replace the start of the line. So using the carrot on, we could start to put in Dentyne if necessary. On we can put in then our extra four spaces from lines to through to 10. In this case, so much of what we learn with said we can also implement when we're editing files ourselves within the VIE text editor. So we're not just learning. If you like CO to be used externally, we can use it within other commands. Let's take a look now on the command line of our appointive machine of how we can implement some of these commands within Fi. So you could see now we're on the command line and let's give ourselves I filed to work with. So we become familiar with the mtp dot com. So let's copy that across to my current directory through into my home directory, and now then I can edit it with the vie text editor on this, a boon to machine running command Vie is going to run Vim fi improved when we open it up. We could see then that we've got various lumps of text that we've seen as we go through. As we look at it, we could see that we've got the text stats that is repeated a number of times. So on the second line here we've got loops. That's Pierre stats, et cetera, And that's going to appear numerous times through the file. So going so hitting the escape key, taking me through to the last line mode. If I want to replace it throughout the file, I can use the percent operator to specify the whole document. We then use our substitute command and set up then the DL emitters that we're using. We're going to say that we're going to substitute the text stats and then put in our forward slash and we're going to replace it, then with stuff. Why not on clothes off the DL emitters would do it globally, so if it occurs more than once per line. We can make that change on. Then when we hit the enter key, we should be able to see the change go through on Dhe. Sure enough, we got everything now written as stuff and nine substitution Sze Now today with the flick this back, we just use youto undo those changes going back into our last line mode, we can hit the up arrow key so we could see that we've done. If we don't want to affect the whole of the document, then we can go in and specify that we want to work. Perhaps just on specified Rose. So we can say in this case, will have rowed to through two or three or lying to through 23 on we could see. Sure enough, it's only those changes that are implemented and we're down to five substitution sze. But of course there are other things that we can implement. It's less than just go back and choose to delete Line three. So three and D will delete that line when we're looking at making changes. We could also go through an implement those changes through mechanisms other than just the pure line number, so if I now go through, put my pattern matching to try and match online, beginning with stat and cues to substitute. So now, as we go through with substitute again and will indented it in, it's and we could see now is just that line that began with stat that we've indented. Additionally, of course, we can IND end lines where we want multiple lines selected. So again, going back escape colon. And now we'll choose lines 5 37 and we're going to do the same type of Indian tears. Are we looking for the beginning of the line and we're going to replace it with the four spaces on we could see. Sure enough, those lines have bean indented but remembering as well we can refer to the end of the document So we wanted, Let's say, the bottom three limes indented. That's just double check on the line number rings by going through and turning online numbering in our vim editor on. We could do that again just by Goto our last line modes air hitting the Escape, key colon and set number. So with this implemented, we could see now that it's eight through to 10 but we can go through an implement this change by using then the dollar to represent the end of the document. So we go eight comma dollar dollar being the end of the document and then go through an implement our search and destroy or substitution. So as we've gone through now, we've tabbed in those is looks like probably typed five spaces rather than just two spaces. But then that shows is what's available to us. There are other commands are available to us, so let's just take a look at what's available to us in those other commands.
Reading and Writing to Files

[Autogenerated] now one thing that we didn't look at What we're looking at said was the ability to read in and write to files. So let's take a look at that using vai if I go in now with vie and open up my past C S V file. So we looked at this before when we were looking at grip and how we could read in information, print it out and use grip to search it. We could see that we do need to in dent our lines. Okay, will turn the line numbering on so set number says, Turn my line numbering on and you could see that we want to be able to IND end then lines six through 29 so we can go back in here and then right six through 29 And then we should be familiar with this now. So we're going to search the beginning of the line or substitute the beginning of the line with our four spaces. So that's being now done effectively the way that we like. We might take that while loop as an example, we might want to save it as a file to bring in as an example to another while Luke so really treating this is a snippet. So what we're going to do now is choose lines for through to 10 and we're gonna write thes through to We'll just call it a while, which will. D'oh! So we've written that information out and again the same w command is available to you within said, Let's jump out of this now when we go through and let's say it, it's a new file. So we just go buy new on. We might want to write a similar while loop so we can go in and read in. Then from that file on, we could even use our tab completion here. So we see we've read in that violin. Now we've got our sample snippet on again. Even with said we can read information in on. Perhaps we could do that whilst a pending or something like that. So is giving you an idea of what we can do with said, but also just seeing how these clubs could be implemented within our vim. A text editor. So let's move back now into the slides and finish up on this module
Summary

[Autogenerated] So you know, it's that time when we need to summarize what we've looked at all what Danny has achieved in this section. As we go through, we can see that we've been looking out. Archiving catalogs will be looking at trying to keep the vice president of sales happy. We've tidied up some configuration files with them on. We've seen some additional tools that we can use with them that we didn't see with said. So the reading and the writing. Two additional files. And of course, we want to see how satisfied Danny is with the performance so far. So when we're looking at archiving cara catalogs, that's where we're looking at using the additional command with the execute. So that's where we were substituting the beginning off the line with a command. So we started off with things like l s minus l and stat, and then to be able to meet what was required by D I Y r us, we had to then go through and use the Tar command toe, add catalogues through to the archive and then delete the file. Once I had gone through. So we saw then how useful that execute function Waas. And we've been speaking with the vice president of sales. I can tell you he's really happy with what I t. And Danny especially has Bean achieving. We kind of turned things then a little on the head. We were saying that we were getting said to run commands, but we could see that from with inside our vim text editor, we can use at least said, like commands to be ableto edit our files. So if we're already interactively editing the file, then using ah, substitute and replace could be really useful because we're we've got the file open we don't need them to use said. But we might simply need to change the name of a company or change the customer's name or whatever within the current file that we're working with. We also saw that it's quite easy to invent the lines when we're working through scripts and additionally, we went through seeing how we could use the W command to write our output on. We're saying that we could start saving elements are off our code snippet, then going into a new file and using the our command for Reed to read in that snippet file on. We did that with, Ah, while Lupin. We saw then how we could write a while, loop out and read it back into a new script to start creating our own snippets, something that you might expect from a graphical i d. But seeing how we can emulate the same thing from our command line text editor. So even though we spent a little bit of time running through, said and said is going to be really useful, especially remembering that we were able to carry out our remote edits. So simply using said, but sometimes said is not going to be enough on some things that we've seen have been quite complex, remembering back to where we were looking at things like our substitution groups and upper casing and lower casing. Sometimes he's going to be easier to implement it with a walk. So Grandpa was Good said was most satisfactory. But neither is enough Not for Danny. He always striving for something more for the best tool on the best tool to suit the job in hand. And this is where walk becomes awesome. At this stage, we leave said behind on, we enter the realms off walk and don't think the walk is just gonna be something that's going to be terribly complex. We could see Howl could make some of these task that we've already been looking at so much more easy, as well as being able to provide so much more functionality. So I see you straight back where we start looking at a walk.
Fundamentals of awk
Introduction to awk

[Autogenerated] welcome to this plural site presentation. My name is Andrew Mallet, and we're gonna look now at the fundamentals of Orc and how we can implement them within the company that we're working with. D I Y r us as we go through, we'll see what Danny is planning to implement. Using Walk. We're gonna see why we're going to use Oak Review The basics of Orc. Remember, we did have a quick overview of this within the introductory module to this course, we'll be looking of the formatting of the employee file and looking at perhaps a better way that we conform at the information rather than using said. And we're also gonna identify a problem that Danny has come across whilst using the last lock command and seeing how walk is gonna help him give a better output to the last log command. So first of all, then why Orc? Isn't it really complex? Isn't it something that I want to leave two experts? Well, no. We're going to see some of the simple uses of Orc as well as how we can use of its Maur complex internal functions to be able to make life more easy for us as administrators, we'll start seeing that we can create headers with the begin block on. We can create summary information within footers using the end block. It's his own programming language and we have conditions and loops as well as internal functions. Having a quick review off the basics, we could see how we can start printing information from our E T C password file. At least we'll know we're gonna have a password file on our limits Hosts. We're taking a look here at a control file so much in the same way as we can use a control file within, said Orc could also use the control file. We've called this users dog Cork to say that we're working really with users within our password file and to give an idea of its purpose. Having walk as the last part of its name, we can see the use off the begin block making up the header. We could also see within the summary block or the end block making use off the N. R or the number of records variable. We're also using within the header information within our begin block. We're using a file separator to specify that we're going to use Colon as the DL emitter for our file that we're looking at. We execute this code with Orc minus F reference in our or file in the TC password file. The begin block is processed just once in the end, block is just processed once. Where is the unnamed block in the middle where we're looking at printing dollar one that's then printing out the user name from the input file that we've just referenced E TC password. Perhaps we just want to print users where the user I d is greater than 499. So we could take a look at the users. Don't walk file that we're using on. We could see that we're printing something very similar to before within the begin and the end block, but adding now a range in front off our main block. And we're saying dollar three, which is the user I D column is greater than 499 again weaken. Reference this in the same way. If we want to count the number of returned rose rather than the total number of rows in the file, then we can process it in this manner so you can see we've got the same type of information. However, when we look at the end block now we've got total users equals on. Then we're referencing the count variable. The count variable is utilized within our main block where we incremental it the first time round. It won't have a value but subsequent times. Then we're goingto increment it the n r variable. If we use that would show me the total number of lines in the file where using our own count variable as in this case, we're only showing lines that we process that match the beginning string, then Groot. We can use the semi colon both in the begin block the main block at the end block, and it is just used to separate commands out. First, move that into the command line and get to know a little bit of these orc functions
The Basics of awk

[Autogenerated] now I don't think it's going to take too much to realize that we're back on the command line of our urban to $40 4 desktop on. We're going to start our introduction to the fundamentals off walk. We're gonna begin this by taking a look at a configuration file we've already gone through and created. But as you can see, we've just got our unnamed the main block in here within the brace brackets and the one statement where we're printing out the first field. Now, if we try and utilize this weaken go orc minus lower cave F representing the file that we want to read in. And then if we try it, let's say agaves. Three e TC passed word fire. Remember that this is delimited by Coghlan's. So because we're not noticing than a space in here, which is going to be our default delimit er. Then, of course, we print out the first field, which is the complete line. Now we could modify this a little bit by changing the field separator so we could specify with a capital F that the field separator is going to be a colon and now we start seeing the behavior the way we would like. But perhaps if we're always going to be working with the same file, we can embed this'd into our configuration. So if I go back to my control file, they uses dot Orc, What we going to do now is insert a new line and we're gonna put in a begin block. We're always gonna have the brace brackets. This is gonna be relatively short. So I'm gonna keep it onto a single line on we can have then r f s equals on we can put in then the colon. I'm gonna put another statement here, so I'm just going to separate the statements with us. Semi colon on. We can print out, then a little bit of header information. We might as well do this as we're already here. So we're gonna go in and then specify that this is the user name and close off my brace brackets. Was this shut? Now will we go back and take a look? So even without the minus capital F now where we reference the field separator were able to get the correct information because we've built it into our control file Yeah. No, we're not really seeing the output from the header information. So if we go back and then just send this through to a paging program, you can see that the first line of output is in showing the user name. We can also build in some information relating to the number of rows that we process. If we go back into our users startalk and now we go to the capital G, takes us to the end of the line and then use Oh, to insert a new line we can add in an end block again. This is gonna be relatively straightforward. So I'm gonna keep it all on one line we're going to print than the total uses. You can have some kind colon or something like that in a space after that, close off, off quotes and then we using an internal variable so they don't have the dollars in front of them. This is just the number of records. So now when we go through clothes off that and have a look course, this is going to be easy to see because they're coming right down in the bottom. But we then get paged through. We've got in our total users being 40 so you see how we can start building this up? But of course, there are a lot of users here that perhaps we don't need to see. We only really interested, if you like in our standard users, so we can go back into our control file here in front, off the main block. We're now going to put a range. So I'm going thio insert, and we're going to choose them where Dollar three on Dollar three represents. The third field on, we could say, then, is greater than 999. Lot just depends whether you using a scent off system where your standard users started 500 often on Sousa and a boon to systems standard users start at user I. D. 1000. So we're really requiring that the user i D in this case is greater than 999. So let's now go through and take a look at the output that we get with this. So if we go back, let's not worry about the less command at the moment. When we go through on, we've now got just our standard users because they still see that we processed each line and our total users is still showing as 40. If we need to just say, well, how many users do I have above 999? Here is easy to count, but obviously in a more practical situation, it's not going to be as easy to count. Let's go back into Fi, and what we're going to do here is just add another line so we can go through and insert the semi, Colon said. Just separating out our command. And then we can increments the count variable. If the count variable doesn't have a value to begin with there we extension Kate the count variable and increment it, so we'll start at zero. So the first time around it's going to be one and then two and three etcetera. And then, of course, when we're looking at the processing information here, we can then specify that we want to work with that count variable. So we just print out the count. Once we're happy with that, we save it and then we go back and run the Orc command again and we could see now that We've got five users who matched the criteria that we put through. So if we take a look back then at the user's orc, we could see how pretty easily we can start creating some usable information and retrieving information that we want. Of course, here we looked at it where we were searching for user's who had a certain user, I d. But we could also go through and search, say, against the primary group I D. Where we were having shared groups or something like this. Or of course, we could just revert back to searching for something where the name began with a certain character. So just to prove that this can be manipulated around, let's go through now and changed this. And if we go through and put are normal pattern match, so are 24 statues on within their forward. So she's we might say that we want our user to begin with an s. So we're saying the lying that begins with an s now as we go through and saved the information, clear it out, run our orc command again and we can see now that all of these uses start with an s. And of course, we could see that we've got seven of them. So we could start moving this forward to other applications where we needed to search, maybe for product names or something like this. So it becomes very easy to be able to manipulate and create the search is that you want and, of course, hold them in the control file so that they're easily Andi reliably repeat it. Now let's return to the slight. Because Daddy's had an idea now about how we can simplify the formatting of the employee file. So let's go back into the slides and take a look at that.
The Employees Application

[Autogenerated] I'm sure we will realize that we ask any tradesmen, they'll tell you. It's all about choosing the correct tool for the job. When we look at the all code, which is going through a Napa casing, the first field of our employees file and lower casing the second field, a cz well as just printing the third field as is we can see how much more simple the code appears, then our original attempt to using set. So with said in the substitution groups, this was working, but it wasn't necessarily going to be a great ongoing solution, I think, especially when we come back to look at this, maybe in a month or two months time. I certainly know which code I would prefer to see within my files. So we're going to see even though the or can be quite complex. It also could take complex scenarios and break them down to be quite easily digested. Let's move out now to the command line and see how we might implement this newer on Dhe debated Lee Better solution. So back on the command line, then let's remind ourselves of the employees file on. We could see that we've got these mixed case names. So the first field is a surname, and the second field is the first name on. We want to ensure in our output that we've got an uppercase surname and a lower case first name. We will include in our output as well that third field, which is a Social Security number. Now again, we haven't run through and written our code yet. There's going to tidy this up. But we did see that we could quite easily with our regular expressions, filter out the incorrect Social Security numbers. So we work with his template file to begin with and just see how we go now. We're not gonna need any huge complexity in this, so I'm going to leave it as such with just no walk file and just all from the command line. So we're going to use the minus capital F option to specify our field delimit Er, of course, this is a comma separated file. We can then go through an open up our quote. I'll close them straight away so we don't forget and then open up our main block it again. I'll open and close that within the main block. What we're going to be implementing, then, is a simple print statement to begin with. Just to see this working, we're We're gonna print dollar one dollar too, and dollar free. I'm using a comma two separated, nothing to being a comma separated file. But I want these to be a separate field rather than all bunched together if I don't specify their output file separator than we use the default of a space. So when we then jumped through an ad in our employees file, we can see then that we get the three fields printing out and they're just space separated. But I could use a kn o f s variable and set that in my begin block if I needed to control the output. So we need to upper case this So it's clear the screen set some spaces and just go back up. So returning to our code and it couldn't be more simpler of just running to upper and then specifying field one and making sure that we spell to upper correctly and in the same vein we can go to field, too, and used to lower and make sure that we surround this in the parentheses. With that in place, then we can hit the enter key. And now we've got everything correctly formatted and in a relatively simple manner, certainly for long term maintenance. This coat. Generally, I find if I look two or three weeks later after having written something, it is not written. Titley. I have no idea what's going on. Well, we can see pretty much anybody should be able to take a look at that and have a great understanding, with or without comments off what that code is doing. So let's move now back to our slides and take a look at how we could start to prepare a better output from the last log command.
The Problem with lastlog

[Autogenerated] now, of course, I really don't know how often you run the last lock command. But if you have run the car command recently, you're probably going to be well aware of the amount of output that you're going to get with it. It's gonna print the last log in time for all of your user accounts listed in the E T C password file. However, there's gonna be an awful lot of accounts that have never logged in on there. Of course, go to show they're gonna be your service accounts. Those sorts of things where perhaps even log in isn't really permitted with the log in shell maybe being set to slash been slash false or something similar to that. Also, of course, many of your user accounts will be logging in, so they're gonna show a last log in time of somewhere, perhaps even that day. Now there are switches to last log to help you form at it. But even with those switches, the output really doesn't do the command justice. So Danny has identified these limitations and decided that walk is going to provide a solution to give the sort of report that he is looking at. So we're gonna run through and see what Danny is planning. He's creating, then a control file. For all we could see, the last logged on ork Andi, the range that we put outside of the main block is negated or reversed. You can see outside of the parentheses that includes the or statements with the vertical bars we've put then the exclamation mark. So the exclamation mark reverses it. So we don't want to work with lines that contain the string never logged in or lines that begin with user name. That's just gonna be the title line. And we don't want to work either, with lines that begin with root. That's going to be our route account, and we don't need to print details on that. The brace bracket then opens up our main block on we increment and instance, she ate the counter, and we're going to use that as we saw before. Within our summary block, we can see here that we're looking at the NFL variable so very much like the number of records we've got the number of field for each record, depending whether or not the user has logged in locally or remotely virus s H would depend whether or not we have eight fields for your local connections, such as for Dev T T y one deaf t t y tu et cetera, or nine field where we're logging in, most usually fire. Sshh. Using the putty client off perhaps even a Linux next s stop. So if we've got the eight or a local connection, then we're going to print out Fields 154 and eight. But we form at the output. So instead of using print, we using print f we're saying that we've got eight character strings to character strings, three character strings, four character strings and don't forget with parent if we have to include a new line if we want a new line, else is going to be a nine field. So we print out field 165 and nine. And that's really the pertinent information in both case, whether it's eight field or nine field about the user log in details with in print our summary block. So we've got a nice little separator with all of the equal signs making a double line and then a new print command underneath that that shows the number of users processed, and this is where we utilizing them, the count variable that we were incremental within our main block. So that's going to be roughly what our code is going to look like. So let's now work at the command line and see the standard output from last log and how we can make improvements on the last lock output.
The awk Solution to a Better lastlog Output

[Autogenerated] so back then on the command line as they're born to machine. If we just use the last log command as is, we could see that we get an awful lot of out. But including ah, lot of service accounts that, of course, have never longed in. There's just clear their screen. If we go through and do something like last log mind issue and then say, Just put the route user in, we could see. Then the route user is again one of those user accounts that have never logged in. But even if the route user had logged in, maybe we don't want to include the output. We're also seeing that the beginning of the header line starts user name. Hence, when we start excluding things like User name and root, your understanding where we're com coming from. But of course, there are Arthur accounts that haven't logged in. So if we go through and use that, say, like the whoopsie account, So the whoopsie account, however terrible it sounds, is on error reporting toe on, we could see the walks the account again has never loaded. It's a service account. It's a little bit like the old Doctor Watson, that we usedto having a window's environment if you're used to that. Now, of course, there are other switches that we can implement so I can go 70 something like a last log and then minus B and then say 90 Show me accounts that haven't logged in within the last 90 days. But the downside of this a gate is I just got an awful lot of never logged in that account. So if I did have accounts that had genuinely not to logged in within the last 90 days and word service count, I'm not really going to be able to see them too well. Like most of these tools, there is an online help that's available so I can go through and see what's available with the reason, really the solution that I want, where I don't want to be able to print those accounts that have never logged in. So for this, this is where we can utilize walk just a little bit on when we go through and take a look. If I go through now and take a look at the last log or GE very similar to what we saw previously within the slight. I'm just including here a begin block where we're printing out our own header information within the main block again, we can see we're excluding, never logged in user name and root on our variable that we're working with now we've just abbreviated to see Auntie rather than a full camp songs. It matches where its first used an incremental with the end block. It's going to be fine as we go through. We're looking then, for the number of fields being equal toe eight. That's RS statement, and that's gonna represent our local connections. The L statement then is going to represent where the feel count is nine is either going to be eight or nine on the nine will include an I. P address or host name from the client that they're connecting with rather than the local connections. And then we process the information in the summary block much in the way that we saw before. So of course I can then go through and do a last log, have the output pipe through to our application file. So through to ork and then minus f on, we're gonna reference in that last longer file on. We can see now that we've got our formatted output. So these are user accounts who have logged in Now again, my systems don't really tend to live very long, so I don't have a huge amount of account activity on this. But if we do something like this where we use last long minus B on will say that I haven't logged him within the last seven days than it reduces the output. Just a Bob, Sally and Gretchen on we could see with the information that we formatted. We've put it then in a nice format so we can see exactly the information that we want. So this is a say that potentially becomes a better use of the last log. I'm really making use off or get its very best. But for the moment, let's move back now into our slides and wrap up this section
Summary

[Autogenerated] Well, just look at this. Wouldn't you know who's won? Employee of the month? That D'oh, Why are us? It's Danny and probably well deserved. If you see what he's been doing over the last few modules and improving the reporting situation just recently, we've seen what we can do with organ. What Danny is implementing at D I. Y r us, we've looked a couple of main applications. Firstly, Danny reviewed the formatting of the employee information on. We've seen that for longevity is a lot easier to use Orc to form at the information on right this back then to the employee file, we looked at using the internal functions to upper and to lower, and that's certainly simply provide the process from where we had been using our said substitution groups. With that in the bag, we moved on to something that Danny needed for his own I T department to be able to produce a better last log. There were an awful lot of an account that we perhaps hadn't been told about that had left the organization, so we should have a review process where we can go through at least once a month and check for accounts that haven't logged in within the last 60 to 90 days or whatever we feel is appropriate. But the last log it'll just didn't really seem up to it with so many system accounts that never logged in. So Danny implemented using orca simple mechanism where we could filter out the information on only show those accounts that are valid accounts with a log in Dayton time working. Then, with last log, we can produce good results showing the number of users that meet the criteria and also the last log in date rather than the date and time. Of course, this is only the very basics off Orc, and there's vast amount that we conduce with Orc, all of which we want to keep very practical. So we're moving into the next section now, and within the next section, we're gonna base our application around passing XML data. So we're gonna move away from our standard product catalogues, a move towards XML based catalogs and seeing how all can process that information for us. I think you're gonna be really impressed with how easy we could read individual XML blocks from our configuration files
Displaying Records from Flat Files Using awk
How to Report from XML Files with awk

[Autogenerated] welcome to this plural site presentation. My name is Andrew Mallet, and we're going to take you through displaying records from flat files using Orc on by flat files. We really mean XML data. So we'll be looking at how we can use or to traverse our XML files. Of course, XML is tagged data on. We can choose their nor to display the values that we want. The idea really came about by looking at virtual host configuration files within Apache. But we'll also be looking at how we can move our product catalogues towards XML and be able to search those product catalogues. The secret behind displaying this tag data is the fact that an ORC record will normally match each line within the file or the standard output, using a little imagination those and coupling that imagination with the R S variable or our record separator, we can make our record separator to new lines. So essentially we're matching the end off a record towards a blank line. Once we've done that with enable to search for data and display the complete record for the searched data as we outlined before, the idea really came from displaying Apache Virtual host details As we conceive, though you might not describe it as XML dater, it is essentially tact. So if I wanted to search for the virtual host example toe or if we treated the whole virtual host line, that starts with the opening tag and closes with the closing tag. So we treated this whole data as one line than searching. For example, dot or GE is going to return the complete virtual host configuration for that server. We just need to make sure that each of virtual host itself is separated from another through a blank line, and then we can use an orc file such _____. Here. We're calling it X M Elder Orc Within the begin block, we specify the new record separator that we want to use and to new lines. We're searching dollars zero, which represents them the whole line for our search string on. We passed the search string in at Runtime so you can see we have search equals example, so it becomes very easy, then to extract the example toe or record using orc. Now should we find that we have too many blank lines? All we don't have blank lines than we can either insert the blank lines or remove them. What we might end up doing it. She delete all blank lines from the file to begin with and then ensure that we append blank lines after our virtual host records. Let's begin this tour by taking a look at configuring our virtual host data, making sure that we have the correct blank lines and then carrying out our search.
Starting with Clean Data

[Autogenerated] So we've comfortably installed ourselves again on the command line of the upon to 14.4 desktop on. We're going to take a look now at how we can start reviewing virtual host information. But to be able to do that and to be able to extract that data using Orc, we need to make sure that we've got it in the format that we want. So just take a look at our virtual host file at the moment. So when we take a look at this, the problem I can see here is that after this first document root entry, we've got a clear or blank line there on. We're also missing a blank line between the first and second virtual house definitions. So if we're gonna use a clear and blank line as representing our record, then this file itself is not going to work for us at the moment. Of course, it's sample data is not gonna take a lot for me to go through and edit this file, but you could imagine if this waas many more lines and more lines that needed editing on very much more virtual host or any other tag data. There is no really going to be a useful job, having to sort through thousands of lines of text. Ensuring is in the correct format. But we already know that said is great a tiding up files for us. So we're gonna look at how we can use set to tidy this up, and we're going to begin with deleting those additional blank lines are going through deleting all of our blank lines. So if we go through and it's going to be a matter of simply searching for lines that begin with will say, any amount of spaces or no spaces and then directly followed by the end of the line on, we're going to delete those, we'll double check that this is working and then we'll come back and add in another expression So we'll work on our virtual host file so you can see now that that's all tidy, L Of course we need to upend lines as well. So if we go through now and up arrow key here, we could see that we could just add in our expression. So we want another expression, and this time we're going to be searching for lines that begin with the closing virtual host. We just need to escape the forward slash of the clothes. That should be enough to be able to identify the closing tag. So let's then close off our pattern match that we're searching for on what we want to do is then penned on. We can go through and just simply upend a new line. The easiest way to do that is just with the backslash. Let's go through and check. This is working and you can see now that we've got the one clear line between each virtual host definition once we're happy and I'm pretty happy with output there. So I'm going to go ahead now on put my minus. I option in. So in this case, now we're doing our in the place edit. I'm not worried about a backup file, and now when we go through and get out the virtual host file, it's now looking the way that it should, with a clear blank line between each virtual host and no other additional blank lines to mess up the output with is in place. We're now ready to start to look at how we can search the virtual house file in our Tate XML data. We'll come back and look at that in the next video
Reporting a Single Virtual Host Configuration

[Autogenerated] So we're going to create ourselves a little control file for just to make life a little bit easier. So we got to go into our text editor on We're going Thio, open up then. Our virtual host don't talk. We're gonna insert our data and we're going to begin with our begin block. It seems a very great place to start, doesn't it? We need the begin block here because we're gonna set then our record separator variable. This normally would just be a single new line. But we're going to implement a double new line so we can use that blank line that we've so carefully crafted into our file to make sure then that we can separate the records correctly. That's all that we need. I'm gonna put a semi colon and hereby, don't particularly need one. Remember, we only need the semi colon. If we're putting multiple commands on the same line within going to use dollar $0.0 represents the complete line on. We're going to say that there's a match, so we're gonna use the till the symbol against then our search on we could put anything that we wanted even misspelled. But Of course, we've got to remember it when we put it in on. Then what we're going to do is print. We could print or print dollar one or dollars zero being the whole line dollar one being the first field dollars, zero being the whole line. But print in itself means that whole line or what we really mean now is not really a line, but a complete record. Now we referenced the search that we've created here. We referenced that when we execute all cut run time. So we're gonna save our configuration on. We're gonna run now, fork reference, then our minus F and our virtual host or walk file on we can have. Then our search equals That's a example. I'm not very good at spelling here today, so let's make sure again this matches what we did store in the file. And then, of course, we gained to have this executed against them are virtual host dot com, and we could see that we get the example dork printing out if we go back up and maybe we'll put Penguin in here for the urban penguin. And of course, now we get the urban penguin printing out. So it just shows you what's available in how we can isolate XML records. But of course, now that we're able to isolate XML records, we can also look at then how we can utilize this for our new product catalog. Let's jump back now to the slide and look at how we might do this with our catalogs.
Working with XML Product Catalogs

[Autogenerated] Now, I'm most certain that within your I T department, you're quite used to the whims of the sales and marketing departments as they come up with these huge new ideas. Of course, we have to be able to cater for their brain waves on their latest brainwave is that the product data is now going to be in XML format. In this way, they're able to transfer the data far, easily between their Web applications. So the reality is it does make sense. Before with the flat CS fi fire, we were able to search the data using our past C S V script and then looking at the information with grip and minus capital. A remember the minus capital. A option was in that group of family, where we had the minus capital A to show the lines after the matching lines, minus capital B to show lines prior or before the matching lines. And to show both, we have minus capital C context. So grip minus a and then we were displaying two additional lines after the matching lines was enough for our application before. But with the move to these new XML based catalogs within D I Y r us. We have to look at a new method to be able to extract and display the information. Well, thankfully, this isn't going to be too difficult. The XML based data files, as we've already seen with our Apache configuration files, will be very easily search as Danny has already been working on this for the Apache files. So let's move back now to the comfort of the command line and just see how this is gonna work without product data.
Demo: The New Catalog Application

[Autogenerated] very early on in this course, we were looking at the original product catalogs on. We will see they were C S V files. Later on, we did change them to colon separated files so that we could numerically separate the price columns Should we have up to 1000 in the Thankfully, we had archives some of the catalogue so we can review the previous operation. If I go through now and just take a look at our past C S V script, we just look at a complete catalog file. So if I go through and look at catalog on, then we'll go through and take a look at the tools file. We can see then that we were able to print out that product, the price and the quantity. If we wanted to further work on this, passing the information, then through to grip, we can use minus capital A and two on. Then specify that we want to look at, let's say, a brush. So we go to see the matching lying brush and the two subsequent lines. Afterwards. However, we've now migrated the catalogs across. If we take a look now at the catalog on then the tool dot xml file. You'll see the new format on we using a main product tank to organize each product, and then the name, price and quantity now is showing as stock. To be honest, it was just easier to type stock rather than quantity. Now that we have the information in, we already have an ORC file that will allow us to deal with this data effectively. We can then search very much in the same way that we were before, So we can use Orc minus f. We can specify there in our virtual host our Orc file. And then this time we go through and put in our search and say that we're going to search for a saw. Remember this search string itself was built into the control file Virtual host. Ah, talk as we go through now and take a look then within, ah, cattle and tool dot xml We could see then that we get the sore printing out. And of course, we can go back and change this toe other products so we can go through and put in something like the hammer so we can get through and see this I think we had a screwdriver or something similar in there. So if we go through and put driver, this should be effective enough on. Then we get the screwdriver so you could see how easy it is to be able to isolate the information. Of course, you've got to be able to get the information correctly into the XML format. But being an ex mail form, that does make it then fully transportable between your Web applications and your flat files, if that's what you need. But hopefully we've seen quite a few things within this section. How we can deal with configuration files and display the information with our virtual hosts and then seeing that we can take that same ORC control file precisely the same control file and use it against any other XML data. So we moved it across, then to the catalog and tool XML on were able to isolate products from the catalog. Let's move back to the slides now on. We'll summarize this section
Using Multiple Field Separators

[Autogenerated] Just when you begin to think that everything is going swimmingly, you get a change request coming through from the sales department. They've been looking at the output of the XML data, and they're saying for some people, it just confuses them having all of the tags Now here's just a extract of the data that we saw from the catalogs. So we get the opening product tag and then around each element. We're getting the element name and closing tak. But we can break this XML data down even further, and it does appear that we're going to need to do this now. Using square brackets we can define multiple did limiters. There's a little bit like our character class where we can say it's this or this or this. So if we define, then our field delimit er as being the angle braces both the opening and the closing brace, Then, in which case, every time we come across one of those little diamonds, then that represents the end or start off a field. We have to be a little bit careful with field numbering, and it can take a little bit to understand. But as we go through looking at this output field one then actually becomes before product is like a C S V file that starts with a comment just means that Field one is not populated. Field to is going to be product field free again is unpopulated that between. If you like the closing and opening angle, the name then becomes field for and Field five is the value screwdriver. If it makes it easier, just write the information out of the C s fi file. So everywhere where we see an angle, whether it's an opening or closing angle bracket, just put a camera in so we can quite easily see. Then again, the first field is not populated. Product is the second field. The third field, again is not populated. The fourth field his name. The fifth field is screwdriver and field number six has forward slash name in it. So let's take a look at how this works. By returning to the command line of our abode into desktop
The New Application Release

[Autogenerated] I have already copied across the existing virtual host dot Orc through to our new file that we're gonna be working with the catalog dot orc. Of course, we've got to make some changes through to that file. So if we go through and take a look at their catalog dot orc So this is the exact same file that we were working with before. But we need to implement some changes to be able to break this down. There's go through first of all, that insert a new field separator. So we're gonna have f s on. We're gonna have then equals and put in our quotes and just like the character class or range that we've been using before in our regular expressions we can put in, then that we want to use the two angles doesn't really matter which way or the order that we put them in. But now, any time we come across one of those characters, it will act as a field separator. So again, it's a bit like the character class. We're not looking for both characters, but we're saying either those characters will act as a field separator if they appear to together. Such is in the end and start of attack. Then it's two separate delimit er's and we're gonna add in an output field separator when we got to make sure the vistas equals a space so we can space the fields out on our line within our main block. We now want to print not the whole line, but the fields that we want. So we're going to go through. And first of all, we're going to print out dollar four. So this becomes the name and we're going to print along with that, a colon in the space. You'll see. This is it gets printed out in front of the colon. We want our product name and then we want an output field separator. So we put the camera in, and then we do the rest with the other three elements. We want the name of the element and the value of the element. So we're going to print than dollar eight and a colon into space, and then the value, the next element. So we're going to use then dollar 12 a colon and a space, and then the next value is dollar 13. Once we're ready with that, we can save our change and now we can implement it. So as we go through now, using the catalog dot all searching for our screwdriver from the tool XML we could see we've got name, screwdriver, price five and stock 51. So the fields that we're reading out our field four, field, five, field, eight, field 9 12 and 13. In that order, we could see we've got spaces where we put the calmer in. And that's going to be the output field separator that we're utilizing their and we set that through to a space. But now, even the most grumpiest of sales vice presidents should be quite happy with the output that we've been able to achieve. Let's just check this for everything else. If we go through and say, Try our sore, we could see them that we've got the table saw. We've got the price on dhe. We've got the stock quantity. When we go back up and take a look, let's say a hammer, probably about all we have in the catalog, but we've got then yet the hammer, the price on dhe, the stock quantity. This is absolutely brilliant being able to extract data like this from an XML file. And of course, you can use this as a template to work with any XML data. Let's return now to the slides and just tidy up and summarized this section.
Summary

[Autogenerated] So within this section, we've been taking a look at how we can report from flat files. If you remember back in the days of grip, we do actually have a very good solution where we could report from information within their catalog files. So he had developed the parched C S V script on. We could interrogate the output of that to see the absolute data that we needed using grappa and minus capital A. But you know, that just wasn't good enough for our sales department. They needed to my great their catalog from the well and trusted format of a C s fi file through toe XML files. So now we needed to be able to provide that same level of reporting from XML files. So of course, we needed to redevelop our application within their application. We needed to pass search strings from the command line. So you saw that within the application, whether it was looking at the virtual host tagged data or whether is looking our XML files for the catalogs, that we were able to search for the particular virtual host or the particular catalog entry that we wanted on with that we were utilizing our own variable that we had built into our orc file on. We populated that variable at runtime from the command line. So that was the variable that we had just simply called search. We're able to display our virtual host. That's really where Danny started this idea about reporting on XML based data. He needed to display information relating to an entire virtual host configuration where the virtual hosts were all stored in a single file. So he developed a way that we could use all to be able to separate that data out on the main key to this was replacing the record separator with two new lines rather than the single new line that is the default. So we then implemented this with our new catalogues are catalogs have migrated across to the new format being XML so we could implement. First of all, we tried the same code, we working with the virtual hosts. However, that wasn't really satisfactory because it had all of the tag data as well. So to improve the situation, we found out that we could use more than one field delimit er in this case, it was really useful that we could say that we could use both the opening and closing angle bracket our lesson and greater than symbols. So either one off those could act as our field delimit er And of course, we set that with the F S, our field separator variable Also within our orc file, we ended up using the output field separator just to make sure that everything was going to be space separated. With this all done and dusted, we can move on to our next Orc Challenge where we're gonna be looking at reading from Web logs. So we got to take a live Web log on, be able to read that activity that access Locke will be looking at the access log on reporting on the status code so we could see if there's an abnormal amount of 404 errors or something like that that are occurring or 403 for forbidden errors. Again trying to identify types of activity that perhaps are not normal. We're also going to be able to report on the number of times a particular client has accessed that Web server again. This might be useful for whether we're looking at Web server data. Ah, where clients are accessing our website. We're also going through proxy logs to see how many times particular internal users are accessing certain websites. So there's an awful lot of reporting ability within Orc that we're going to start mining in the next section.
Analyze Web Logs with awk
Understanding the Schema of Web Log Files

[Autogenerated] welcome to this plural site presentation. My name is Andrew Mallet, and as always, I'm here to help guide you through in this case, analyzing your Web access logs using Orc. So the company that we're working with D I Y R us needs to be able to analyze their weblogs. We'll be using riel life examples and using associative arrays within or toe help provide the solutions for us will be working to say, with these real life examples on working with production log files off over 30,000 lines. So we'll be providing you some really ll life examples of how you can work with real files, not just the normal 10 or 20 line files that you might find on. Some training courses will be producing effective reports using orc. Now, when we're talking about processing large files, these tools the tools, such as said an orc, were originally written toe handle large text files from the command line very often, especially back in the day with the UNIX systems. It really wasn't possible toe open some of these files in editors or other forms of readers. So really was a matter of processing them line by line from the command line on working with trivial files are only have 20 or 30 lines, doesn't really do justice to these commands or really show you how you can make most effective use of processing your own large data files. And, of course, they're still many organizations today who are working with large, flat piles now being imported, exported from different systems, and at some stage they have to be read as thes massive flat files. The 30,000 lines is a stop. I also work with companies where we're dealing with millions of lines in files, and it's this type of activity where we really start to benefit by using these powerful command line tools. Danny have shared some access logs from their Web servers with us, and they've got over 30,000 lines, and Danny wants to be able to effectively create reports from these logs. So we need to understand, first of all, the schemer off these Web access locks. So it doesn't really matter whether you're using the Apache http _____ or whether you're using Engine X. The format of the Web access log is consistent across both of these Web surface Sofield one represents the client I P Field to and Field three often just have hyphens in them. They could represent the user I d or the identity i d if that has bean set up. But these tend not to be too reliable. Fields four and five represents the time and the time zone field 67 and eight represents the method. The file that were accessing and the protocol. Usually that's going to be http one or 1.1. Those sorts of things. The method is going to be your get or your post. And of course, the file is the name of the file that were accessing this could be an image, a PHP scripter, a Web page or maybe even something like a C. S s script. The status go to Field nine could be really useful where we want to look at things like 4044 page not found where perhaps we might have some links that are not working correctly or four of three forbidden access. So really important to see some of these. Field 10 is looking at the size using Orc that we can understand some of these log by, say, printing out the I P address from the client printing dollar one or printing the status code from the logs that there's going to be all of our hopefully 200 access. But it could be things like re directions or 300 or so we're talking about before things like the four of four not found or maybe even 403 forbidden as well as other status coz that can be returned by your Web service. But of course this is just printing out the fields, and if we've got 30,000 lines, we're going to see 30,000 lines. But let's just take a look at this to see how effectively that we can deal with this larger data. So we're gonna return now to the command line of our opponent to 14.4 machine and see how this works for us.
Printing Basic Web Access Information

[Autogenerated] So we're back on the, uh, one to command line and we can take a look at our access log. I just go through and count the number of lines to begin with. So I've copied it across just into the home directory. Here on, we could see that the access log has got 30,000 lines or 30,805 lines to be a little bit Maur exact if we want to go through, there's nothing stopping us displaying the content of this so we can even go through and cat out the content of the access file. But of course, as we go through is gonna take a little while to go through all off the element. Just just do a control. See on this just to stop it processing, clear our screen. Get back up to the top. Now, of course, that if we want to start displaying some of this information, we can use our good friend or so we can go through and use Orc on. We can pull out information such airs. Let's print the status coat. If I go through imprint field nine, we're gonna be looking at the status code. Of course, each one of these entries represents a line within the file again when get 30,000 lines printing out. But at least the information is shorter to print out on the screen on from the sample here, we can go through and start seeing that we've got a few successful accesses with 200 on. We've got a few failed accesses with 404 Of course, if I only want to display these, we could use grip. But again, we can go through and emulate that same grip. Look and feel if you like with Orc. So we can go in this, use the up arrow key and we can go through and just put in front off, then our main block, where we can go through and specify that then where Field nine is equal to for 04 As we go through and look at this now, we can then print out maybe the whole line so we could use print or print dollar on what we should be looking at. Here is only where the status code is for 04 says we scroll up here. We could see all of these represent for 04 This is clear this out Going back in, Let's go through now and just take a look at some of the other fields. So if I print out Dollar one, this would be the client I p address. It's clear that information out. So this is then their client I p address that we're seeing if we go back in and even look at things like Field 12. So we didn't go through this within the slides. But when we look at feel 12 we could see this represents then the browser that we're using. So there's a lot of information that we can start to glean. Just depends what we want to be looking at. But the first thing is to understand the layout off the log file. Once we know the understanding off the log file or have a great understanding of our log file, we can use that schema and then determine what information that we want to extract from the file. Let's move now back into our slides and take a look at how we can start producing summary information rather than printing out all off the lines
Creating Summary and Aggregate Information with awk

[Autogenerated] The really great thing that you're gonna find about using Orc to work with your log files is, of course, it is totally customizable. You can choose whichever field or fields you want to include in your report, and you can create reports based around the information that you really need to be able to mine from your data here. We're looking at counting. You need access is by each client I p address. When we look at the orc file that we're using here, the begin block isn't doing too much. You can see we've got a very simple field separator and then we're printing than just log access as a header. Information. Our main work horse is within the main block. We could see there's not a lot of code there, but it's doing on awful lot of access for us here. We've created an array called I P, and it's an associative array. Associative arrays, instead of having an array number, have a new array key name, and that key name in this case is going to be dollar one. So for each I p address that we access, we create a new key for our array based around that I p address for each time we find that key, we increment its value. So it's quite simply written like this I p and then the key name in our square brackets. Are we using dollar one for the key name and then to increment the value we just have our normal operator here, plus plus the value of the key then is incrementally each time we find that I p address. So this becomes a very easy way that we can provide account and you can see the count mechanism is being provided by a four loop that we've created within our end block for I in i p. So the eye is just a variable that we're picking. We can choose that the I p, of course, represents the array. So we're going through each key element off the array within printing the element that we're currently looping with on. We're saying that it's being accessed on. We're printing out its value so he will might see something like I p address seven has accessed 27 times or whatever. If we look at the example off one of these arrays theorem A itself is I p. The key in this case is 192.168 dot 0.1, and its current value is three. So in this way, when we look at the end block, it's very easy to say that I p address one has accessed three times. Now, of course, this gives you an idea, and there's nothing stopping us replacing, Let's say, a dollar one with dollar nine to be able to show the status coat or replacing dollar one with Dollar 12 to show the browser version that we're using, It really is down to us to choose the elements that we want to display. So Dollar nine would display the status coast So 404 whatever on, of course, using something like dollar 12 we will be looking at the browser information. But the best thing to do now is to walk back to Mr Open to get on the command line and make sure you're practicing this at home and go through and create our reports for ourselves.
Demonstration Aggregating Fields from the Access Logs

[Autogenerated] working back at the command line. Let's take a look at how we can create the orc file that will allow us to create this summary information from our Web access logs. So I'm going to go in. I'm going to use Vie. We're going to create a file called Count dot orc or see the names. Not really important. We're gonna have our begin block, and that thing can set any field separators that we want. So I'm going to use. Begin on we can use. Then our field separator would have it working with the default field Separator is a space, so we shouldn't need thio. Add this in. But if you feel it's necessary, there's no reason why you shouldn't explicit Sometimes is a lot better than implicit. We could go in and then specify that we want to print out a little bit of header information so we can make this is elaborate or under elaborate as we want, so we'll just have log access here, which seems fine to me and then close out the brace bracket. As we said, there's not gonna be too much happening on with our main block, but this is where a lot of the work is going to take place. So we're creating an array. We're naming this array in this case to be called I p with setting up then an associative array. So rather than using numbers each time we loop around, the element of the array is goingto have the key name that represents Field one. That's our I P address. So that's the array and key or element it value then will be incremental each time we find that I p address. So it's a simple is that all? An array is is just a multi valued variable, if you like. We're naming each part of the array after the I P address and its value is going to be incremental. Each time we come across that I p address the end block, we're creating our summary information on within here. We're going to set up a new line and we're gonna use a four loop. So four I in I pay. Of course, we have to use the name I P because that represents my array. But the term I I could use any value I wanted there so I could go for fish and chips so long I was using a single word. But I kind of fits because it's short. So it's easy for me to type and you could see how bad my typing can be at times. And it really then suits that were using an I P address. So for each time, then we find an element within the I P arrange. We're going to then print that variable I and then we're going to specify that it has accessed so space and then has accessed. I told you my typing was bad and then we could specify I ___ and then the element number, which is gonna be our variable I and then the text string times and then finish that up. So remember, the comma is just separating our argument. We're then going through space separating all of this. If we don't specify the output field separator, it will use the same as the input field separator. And we've set that to be spaces. So with this set, once we save this information, if we go back now, it's clear out the screen and take a look. So using orc minus f to specify the file that we want to work with so count and then read in our access log. Then we could see for each I p address, we are able to print out the number of times that individual I p address has accessed the Web server so gay we've created a very simple report. We probably would use print after tidy it all up. So it's all lined up nicely. But again, we could see the basics working if we want to go through and perhaps change some of this now, if we go back and look at our count door talk and then instead of using Field One, then we're just gonna change this through to field nine. Now, of course, it's not going to represent an I P address anymore, but it doesn't really matter. The I P term that we use in there is just the name off the array. The wording in our report might not work too well for us now, but you'll see is we go through. We could see each element off those status coats on. We could see that we're getting 23,000. Successful access is 200. We're seeing read erections over 2200 times a lot of 404 errors. So we really need to go through and see what's causing that 404 error. We've got a forbidden 133. And again, we want to really look at what was going on with that forbidden access as people try, perhaps and do things on the Web site that they shouldn't. But as we go through, we could see now how easy it is to create effective reports if we look at the different types of browsers a game back and making our change. So implementing Field 12 clearing this information, repeating it again, and we could see now how many times each one of these browsers has accessed the website. Now, of course, we're still getting a lot of information when we look at the I P addresses and when we look at the browsers here when we're looking at just the status code that was fairly limited in itself. But we might want to they go through see well, which is the most common browser, or which is the most common I p address. So for that, let's go back to the slides to see how we would generate our orc file to work with that information
Calculating the Most Popular Browser

[Autogenerated] working with our summary information, we're now going to see how we can print the browser with the maximum number of access times shown in our log file. Just gonna be a simple matter of modifying slightly the orc file. So you can see here within our begin block. We just changed the header information slightly. We've changed the name off the res. Oh, it now matches the fact that we're looking for the browser and field $12.12 within our end block again. We're just changing the name off the fairy of also be in browser. But we have an if statement now to support it. So we're actually looking each time we go around to see if our variable max is less than the current browser value. If it is, then we are going to set that current browser value to the match. So the number of access is by that browser and Max browser will be the browser name. So we're setting that tow. Whatever the current variable B is with their information then stored, we could easily print the most access was from the Max browser and the number of times Max Times. Let's now move back to the A bun to machine just to see how this is going to come into play. So working now on the A bun to machine, we have already implemented the new ork file. So if I go through and take a look at max dot orc, we could see that it's the same as we have within the slight. It's gonna be a simple matter now of running or GE minus f riend referencing max file. And then, of course, the access log. No, we hit the enter key. We could see the most popular browser was 27,000 times from the Mozilla. So your Firefox is going to be the most popular browser, and that represents probably what we see on a lot of sites on the Internet. So hopefully this is giving you some ideas of how we can create our own summary information. But let's move now back to the slight so we can wrap up this section
Summary

[Autogenerated] I don't know how you feel now, but I've certainly pleased to have been able to work with Danny and his colleagues at D I. Y r us and being able to learn quite a lot regarding Grip, said an orc. In particular, this section was really useful, as we learned howto analyze our data from the Web logs. So we've seen how we can use Orc a race using looping structures within Orc a cz. Well, as if statements. And with all of this, we're able to analyze our Web logs being able to print in each individual I p address each individual browser as well as summarizing which browse or which I p address access the site the most. We've now reached both the end of this course and the end of the module, and hopefully you know a little bit more about, Grip said on ork, as well as perhaps a little bit more about D. I Y. R us and how they implement these tools within their day to day environment. 